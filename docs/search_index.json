[["index.html", "Time Series Methods for Official Statistics Abstract", " Time Series Methods for Official Statistics Alessandro Ciancetta and Daniele Colombo April, 2021 Abstract This book is an introduction to Time Series Analysis methods for Official Statistics. The book is divided into four chapters. Chapter 1 offers a brief introduction to time series analysis and presents ARMA models, with applications in Julia. Chapter 2 discusses the problem of seasonality in time series and some techniques for seasonal adjustment using the RJDemetra package. Chapter 3 is about aggregation and disaggregation of time series having different frequencies, with examples using Eurostat data. Chapter 4 discusses the best-practices in data publication and review, with a focus on revisions analysis. This book has been realized on behalf of Eurostat and the University of Pisa (Italy) as a teaching material for the European Masters in Official Statistics (EMOS). "],["arma-modeling-an-overview.html", "Topic 1 ARMA modeling: an overview 1.1 Time series analysis 1.2 ACF and PACF 1.3 Stationarity 1.4 An important stationary series: the white noise process 1.5 Making a time series stationary 1.6 Wold representation", " Topic 1 ARMA modeling: an overview using Plots using ShiftedArrays 1.1 Time series analysis One of the main problems in statistics in general is that of building good models from observed data. But when we deal with time series this is particularly challenging for two main reasons. First of all, we typically have only a single historical sample available, unlike other kinds of statistical analysis where its usually possible to build a larger sample (if you dispose of the necessary time and money to collect new data). Furthermore, a time series can in theory have an infinite time domain, but in practice we always dispose only of a finite number of observations. Therefore, since in most cases we have to deal with a sample size of \\(n=1\\) (we observe one unit over time) and an uncomplete series, building probabilistic models is particularly challenging. This has required the development of specific techniques, giving Time series analysis the status of autonomous discipline inside the broader field of econometrics. A time series is a sequence of observations \\(y_1, y_2, ... , y_T\\) indexed in time order, each being recorded at a specific time \\(\\text{t}\\). Most commonly, these are taken at successively equally spaced points in time. When this is the case, we speak of a discrete-time series. A series is said instead continuous when the observations are recorded continuously over some time interval. The objectives of Time series analysis are diverse and depend on the specific application, but they are all based on the idea that a time series is a realization from a sequence of random variables or a stochastic process \\(\\{Y_t\\}_{t=1}^T\\). Therefore, a fundamental task is that of shedding light on the probability laws that govern the observed process. These laws can be used to: understand what are the factors that drive the dynamics of the series, that is, the underlying causal relationships; forecast future values of the variable; Often, these goals are relevant also to understand how a variable might respond to interventions. To illustrate the concepts that will be presented we will use the United States GDP series as example: #import Pkg; Pkg.add(&quot;FredData&quot;); using FredData; api_key = &quot;insert_your_api_key_here&quot; # an api key can be requested at https://research.stlouisfed.org/docs/api/api_key.html f = Fred(api_key) gdp = get_data(f, &quot;GDPC1&quot;; frequency=&quot;q&quot;) ## FredSeries ## id: GDPC1 ## title: Real Gross Domestic Product ## units: Billions of Chained 2012 Dollars ## seas_adj (native): Seasonally Adjusted Annual Rate ## freq (native): Quarterly ## realtime_start: 2022-03-14 ## realtime_end: 2022-03-14 ## last_updated: 2022-02-24T13:57:02 ## notes: BEA Account Code: A191RX Real gross domestic product is the inflation adjusted value of the goods and services produced by labor and property located in the United States.For more information see the Guide to the National Income and Product Accounts of the United States (NIPA). For more information, please visit the Bureau of Economic Analysis (http://www.bea.gov/national/pdf/nipaguid.pdf). ## trans_short: lin ## data: 300x4 DataFrame with columns [&quot;realtime_start&quot;, &quot;realtime_end&quot;, &quot;date&quot;, &quot;value&quot;] gdp.data ## 300Ã—4 DataFrame ## Row  realtime_start realtime_end date value ##  Date Date Date Float64 ##  ## 1  2022-03-14 2022-03-14 1947-01-01 2034.45 ## 2  2022-03-14 2022-03-14 1947-04-01 2029.02 ## 3  2022-03-14 2022-03-14 1947-07-01 2024.83 ## 4  2022-03-14 2022-03-14 1947-10-01 2056.51 ## 5  2022-03-14 2022-03-14 1948-01-01 2087.44 ## 6  2022-03-14 2022-03-14 1948-04-01 2121.9 ## 7  2022-03-14 2022-03-14 1948-07-01 2134.06 ## 8  2022-03-14 2022-03-14 1948-10-01 2136.44 ##       ## 294  2022-03-14 2022-03-14 2020-04-01 17258.2 ## 295  2022-03-14 2022-03-14 2020-07-01 18560.8 ## 296  2022-03-14 2022-03-14 2020-10-01 18767.8 ## 297  2022-03-14 2022-03-14 2021-01-01 19055.7 ## 298  2022-03-14 2022-03-14 2021-04-01 19368.3 ## 299  2022-03-14 2022-03-14 2021-07-01 19478.9 ## 300  2022-03-14 2022-03-14 2021-10-01 19810.6 ## 285 rows omitted After retrieving the data, we apply a log-diff transformation in order to obtain the GDP growth rate and make the series stationary. gdp.data[!, :DY] .= (log.(gdp.data[!,:value]) .- (lag(log.(gdp.data[!,:value]))))*100*4; GDP = gdp.data[!, :DY][2:end]; To start understanding the basic features of a specific time series we can use some descriptive statistics. The sample mean is the average value of the observations in the observed time series: \\[ \\bar{y} = \\frac{1}{T}(y_1 + y_2 + ... + y_T) = \\frac{1}{T} \\sum^T_{t=1} y_t \\] The sample variance is a measure of the quadratic deviation of the observations from the sample mean: \\[ \\text{Var}[y_t]=\\frac{\\sum^T_{t=1}(\\bar{y_t} - y_t)^2}{T} \\] The higher the variance, the higher the variability of the observations with respect to the average value. A more specific statistic to time series analysis is the autocorrelation or serial correlation function, which measures how strongly a series is related to its lagged version. The number of lags is usually indicated with \\(j\\). \\[ \\widehat{\\rho}(j) = \\frac{\\widehat{\\text{Cov}}[y_t, y_{t-j}]}{\\widehat{\\text{Var}}[y_t]} \\] where \\(\\widehat{\\text{Cov}}[y_t, y_{t-j}]\\) is the autocovariance function: \\[ \\widehat{\\text{Cov}}[y_t, y_{t-j}] = \\frac{1}{T-j} \\sum_{t=j+1}^{T}(y_t - \\bar{y})(y_{t-j} - \\bar{y}) \\] Note that \\(\\widehat{\\text{Var}}[y_t] = \\widehat{\\text{Cov}}[y_t, y_t]\\). When the values of \\(y\\) at time \\(t\\) and \\(t-j\\) are both above or below the average value, then \\((y_t - \\bar{y})\\) and \\((y_{t-j} - \\bar{y})\\) have the same sign and \\((y_t - \\bar{y})(y_{t-j} - \\bar{y}) &gt; 0\\). If this happens for most observations, the series shows a positive autocovariance, \\(\\widehat{\\text{Cov}}[y_t, y_{t-j}]&gt;0\\). On the contrary, when \\(y_t\\) is above the average value and \\(y_{t-j}\\) below (or vice-versa), then \\((y_t - \\bar{y})\\) and \\((y_{t-j} - \\bar{y})\\) have opposite signs and we have \\((y_t - \\bar{y})(y_{t-j} - \\bar{y}) &lt; 0\\). If this happens for most observations, the series shows a negative autocovariance, \\(\\widehat{\\text{Cov}}[y_t, y_{t-j}] &lt; 0\\). If there is not a systematic pattern between the observations in the time series and their lagged values, then the sample covariance will be approximately zero. function samplemean(Y::Array) T = length(Y) mean = sum(Y)/T return mean end; function autocov(Y::Array, lag::Int64) T = length(Y) lag1(x; j = 1) = vcat([NaN for i in 1:j], x[1:end-j]) Ybar = samplemean(Y) DeltaY = Y .- Ybar DeltaY_lag = lag1(DeltaY, j=lag) p = DeltaY .* DeltaY_lag autocovariance = sum(p[lag+1:end])/(T-lag) return autocovariance end; function ACF(Y::Array, lag::Int64) autocov(Y,lag)/autocov(Y,0) end; function ACF_plot(Y::Array; lags = 14) T = length(Y) ACF_vector = [ACF(Y, k) for k in 1:lags] bar(ACF_vector, legend=:topright, xlabel=&quot;Lags&quot;, title = &quot;Auto-Correlation Function&quot;, titlelocation = :left, label = &quot;&quot;) zero_pivot = 1.96/sqrt(T) plot!(seriestype=:hline, [-zero_pivot, +zero_pivot], linestyle=:dash, alpha=0.5, lw=2, label=&quot;Statistical zero interval&quot;) end; #examples x = randn(1000); samplemean(x) ## 0.019338841056328125 autocov(x,11) ## 0.009241265188276216 ACF(x, 7) ## 0.016061992109006745 ACF_plot(x) Instead of using simulated data, we can look at these summary statistics for the GDP series: samplemean(GDP) ## 3.0448029732505866 ACF_plot(GDP) These simple statistics give us some insights about the GDP-growth time series. The sample mean is 2.92, meaning that the average GDP growth in the considered period (1947:Q1 - 2020:Q2) has been 2.92%. The ACF plot shows that the series is positively correlated with its own first two lags. This means that todays GDP tends to have the same sign of last quarters and last semesters GDP. 1.2 ACF and PACF The Auto-Correlation Function (ACF) is a very useful statistic in time series analysis. It measures how much the time series is correlated with its own lagged values, that is, how much the present values of the series moves together with its past values. For example, consider the price of some good. It seems reasonable to think that todays price \\(P_t\\) is some way related to the price of one month, one quarter or one year ago: \\(P_{t-1}, P_{t-3}, P_{t-12}\\). This is an important idea in time series: past observations can embed systematic information about the future realizations of the same process. However, the ACF measures how much lagged values of a series affect successive values both directly and indirectly. That is, it measures how much todays price is affected by the price of two months ago via the effect on last months price as well as the direct effect of the price of two months ago. To be more precise, it doesnt disentangle between the two relationships \\(P_{\\text{Jan}} \\rightarrow P_{\\text{Mar}}\\) and \\(P_{\\text{Jan}} \\rightarrow P_{\\text{Feb}} \\rightarrow P_{\\text{Mar}}\\). Nonetheless, its often interesting to capture only the direct effect. Consider the case of a fair coming to town every two months, which causes the supply and possibly the demand of a given good to change. In this case, there is a direct relationship \\(P_{\\text{Jan}} \\rightarrow P_{\\text{Mar}}\\), which embeds the information about this event, and \\(P_{t-2}\\) might be a good predictor of \\(P_t\\). To detect these direct effects, we can use the Partial Auto-Correlation Function (PACF). The idea here is to get rid of all the indirect effects and isolate the direct impact that a past value has on the present. To do that, we just need to estimate a simple linear regression of the series on its lagged varsions: \\[ P_t = \\beta_0 + \\beta_1 P_{t-1} + \\beta_2 P_{t-2} + \\varepsilon_t \\] From basic linear regression analysis, we know that \\(\\beta_2\\) isolates only the direct effect of the second lag. Therefore, the PACF between the series \\(P_t\\) and its second lag \\(P_{t-2}\\) is exactly the coefficient of the regression, \\(\\beta_2\\), which can be estimated with OLS as \\(\\hat{\\beta}_{OLS}\\) Lets see this in practice. ## PACF ## Simulated data generating process  = 50.0;  = 0.7;  = -0.85; P = 50.0; P = 50.5; T = 1000; P = vcat(P, P, zeros(T)); for t in 3:length(P) P[t] =  + *P[t-1] + *P[t-2] + randn() end ## Generated time series plot(P,color=:blue, lw=1, alpha=0.6, label=&quot;Price&quot;, legend=:topright, xlabel=&quot;Months&quot;) ## OLS estimation of the PACF ## 1) Building the regressors matrix P_1 = lag(P, 1); # First lag P_2 = lag(P, 2); # Second lag constant = [1 for t in 1:length(P_2[3:end])]; # Intercept X = hcat(constant, P_1[3:end], P_2[3:end]); # Regressors&#39; matrix y = P[3:end]; # Dependent variable ## 2) Formula of the OLS estimator. The result is a 3-elements Array [, , ] _hat = (X&#39;*X) \\ X&#39;*y ## 3-element Array{Float64,1}: ## 50.029846734840305 ## 0.6985763269629239 ## -0.8490354147432188 ## PACF for the second lag _hat = _hat[3] ## -0.8490354147432188 Note that in the previous code we introduced the lag operator (L), a useful device that has any applications in Time series. This is a function that maps every value of the time series in its previous values: \\(L Y_t = Y_{t-1},\\) \\(L^2 Y_{t} = Y_{t-2},\\) \\(\\ L^3 Y_{t} = Y_{t-3},...\\) The PACF plot is a useful device: ## PACF plot function PACF_plot(Y::Array; lags = 14) PACF_vector = zeros(lags) T = length(Y) lag(x; j = 1) = vcat([NaN for i in 1:j], x[1:end-j]) for i in 1:lags constant = [1 for t in 1:T-i] regressors = hcat([lag(Y, j = h) for h in 1:i ]...)[i+1:end, :] X = hcat(constant, regressors) y = Y[i+1:end] _hat = X\\y # Shortcut for _hat = (X&#39;*X) \\ X&#39;*y PACF = _hat[i+1] PACF_vector[i] = PACF end bar(PACF_vector, legend=:topright, xlabel=&quot;Lags&quot;, label = &quot;&quot;, title = &quot;Partial Auto-Correlation Function&quot;, titlelocation = :left) zero_pivot = 1.96/sqrt(T) plot!(seriestype=:hline, [-zero_pivot, +zero_pivot], linestyle=:dash, alpha=0.5, lw=2, label=&quot;Statistical zero interval&quot;) end ## PACF_plot (generic function with 1 method) PACF_plot(P) As we can see from the plot, only the first two lags are significantly affecting the present price. Indeed, the other estimated prtial autocorrelations are not statistically different from zero: we have no evidence that prices from 3,6,12 months ago affect new prices. Since in this artificial example we know the true data generating process (as we built the series) we can see that the estimates reflect pretty well the true relationships between the variables and that the PACF plot shows clearly that only the two first lags are involved in price formation. We will get back on this when discussing auto-regressive \\(AR(p)\\) models. Lets apply this function to the GDP series: PACF_plot(GDP) This suggests that the first lag might be a good predictor of the GDP series. 1.3 Stationarity Even when focusing on models which postulate simple relationships involving only first order and second-order moments, it is necessary to assume that some features of the probability law underlying the model are stable over time. This is because if the moments of the process change arbitrarily, it can be difficult to forecast future values of the series, since this would mean that the data referring to different points in time in our sample are realizations of random variables that can differ substantially. Stationarity is an assumption about the stability of the properties of the stochastic process. The simplest type of stationarity is weak stationarity, also called covariance stationarity. Definition (Weak stationarity): A time series \\(Y_1, Y_2, Y_3, \\dots\\) is stationary if \\(E[Y_t^2] &lt; \\infty\\), for each \\(t\\), and The expected value of any observation is constant and does not change over time: \\(E[Y_t] = \\mu\\). The autocovariance is constant, given the number of lags. That is, the autocovariance is independent of \\(t\\): \\(\\gamma(h) = \\text{Cov}(Y_t, Y_{t+h}) = \\text{Cov}(Y_{t+k}, Y_{t+k+h})\\), \\(\\forall k \\in \\mathbb{Z}\\). A first practical way to check for stationarity is to look at the plot of the time series. Lets have a look of a counter-example with non-constant mean. ## Non-stationary time series ns2 = [sqrt(i) + randn() for i in 1:400]; plot(ns2, color=:blue, lw=1, alpha=0.6, label=&quot;&quot;, legend=:bottomright, xlabel=&quot;Time&quot;, title=&quot;Non-constant mean&quot;, titlelocation = :left) The GDP series in levels is also non-stationary: plot(gdp.data.date, gdp.data[:, :value], label=&quot;&quot;, xlabel=&quot;Time&quot;, ylabel=&quot;GDP (Billion USD)&quot;, legend = :bottomright, ylim = (0, 20000), yformatter = :plain) Covariance stationarity assumes that only the first two moments of the time series are time-invariant. This assumption is crucial for linear time series, such as ARMA processes. Stricter concepts of stationarity impose time-invariant conditions on the joint distribution of series. Such conditions may be required when dealing with non-linear relationships among variables at different times, which go beyond the scope of this course. If the time series \\(Y_1, Y_2, \\dots\\) is weakly stationary, we can write \\(\\gamma_t(h) = \\gamma(h)\\) and the autocorrelation function becomes \\[ \\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} \\] 1.4 An important stationary series: the white noise process Definition (White Noise): A process \\(\\varepsilon_1, \\varepsilon_2, \\varepsilon_3,\\dots\\) is called white noise if it has mean zero, constant variance and it is not autocorrelated: \\[E[\\varepsilon_t] = 0;\\] \\[ \\gamma(h) = \\begin{cases} \\sigma^2 &amp;\\text{for} \\ h=0 \\\\ 0 \\quad &amp;\\text{for} \\ h&gt;0 \\\\ \\end{cases} \\] Lets have a look at a couple of counter-examples: ## Not white-noise nw1 = [cos(t/20) + randn()/5 for t in 1:400]; nw1_plot = plot(nw1, color=:blue, lw=1, alpha=0.6, title=&quot;Presence of autocorrelation&quot;, titlelocation = :left, label=&quot;&quot;, xlabel=&quot;Time&quot;); nw2 = [cos(t/50) * randn()/100 for t in 1:400]; nw2_plot = plot(nw2, color=:blue, lw=1, alpha=0.6, title=&quot;Variable standard deviation&quot;, titlelocation = :left, label=&quot;&quot;, xlabel=&quot;Time&quot;); plot(nw1_plot, nw2_plot, layout=(2,1), size=(700,500)) These series cannot be white noises because they show a clear pattern over time, either in terms of autocorrelation or standard deviation. This roughly means that there is some information in the data that could be modelled to grasp the regularities of the process. For instance, in the first example, the value of the series at one point in time gives us a good idea of what the value will be in the subsequent period, which is not the case when dealing with white noises. Indeed, the most important property of the white noise is that it is not predictable. This is a main feature, which allows us to consider a white noise time series as a sort of yardstick for our analysis. Most often, in Time series econometrics we treat series as composed by a predictable signal and a noise. If we manage to build a model that captures almost all the signal and all we are left with is just an unpredictable white noise, then our model is likely to be fitting the data well, in the sense that it efficiently uses all the available information. 1.5 Making a time series stationary In many cases, it is possible to reduce a non-stationary series to a stationary one by applying some simple transformations. For instance, consider the following data generating process: \\(y_t = \\beta_0 + \\beta_1 t + \\varepsilon_t\\). The series is clearly non-stationary, as we can deduce by the increasing trend shown in the plot:  = 2;  = 0.4; y = [ + *t + randn() for t in 1:100]; plot(y, color=:blue, lw=1, alpha=0.6, label = &quot;y&quot;, legend = :bottomright, xlabel=&quot;Time&quot;, title=&quot;Non-stationary time series&quot;, titlelocation=:left) A strategy here is to apply a first-difference transformation. Let \\(\\Delta y_t = y_t - y_{t-1}\\). By substituting the expressions for \\(y_t\\), we can write \\(\\Delta y_t = (\\beta_0 + \\beta_1 t + \\varepsilon_t) - (\\beta_0 + \\beta_1 (t-1) + \\varepsilon_{t-1}) = \\beta_1 + \\varepsilon_{t} - \\varepsilon_{t-1}\\) which has indeed constant mean: \\(E[\\Delta y_t] = E[\\beta_1 + \\varepsilon_{t} - \\varepsilon_{t-1}] = \\beta_1\\). y = y.-lag(y); plot(y[2:end],color=:blue, lw=1, alpha=0.6, label = &quot;y (First Difference)&quot;, legend = :bottomright,xlabel=&quot;Time&quot;, title = &quot;Transformed time series&quot;, titlelocation = :left) Making a series stationary before proceding with further analysis is good practice since its a substantive assumption upon which many models are based on. For the GDP series a convenient transformation is log-difference, which not only makes the series stationary, but also allows to interpret the resulting variable as GDP growth. We already applied this transformation in the introductory paragraph since ACF and PACF are meaningful only when applied to stationary series. plot(gdp.data.date[2:end], GDP, label=&quot;&quot;, xlabel=&quot;Time&quot;, ylabel=&quot;GDP growth (%)&quot;, legend = :topright) 1.6 Wold representation Its worth mentioning a very important result in time series, the Wolds decomposition theorem, which states that every covariance stationary process (with mean zero) can be written as an infinite sum of current and past white noise terms with weights that are independent of \\(\\text{t}\\). The basic intuition of the Wolds decomposition theorem is that if a series is covariance stationary, we can always think of its every new observation as influenced by past white noise shocks and a current shock; and the magnitude of this effects dependent on the lag between each shock and the observation, as expressed by the coefficient \\(\\psi_j\\). $$ \\[\\begin{aligned} Y_t &amp;= \\sum_{j=0}^\\infty \\psi_j \\varepsilon_{t-j} + \\eta_t \\\\ \\end{aligned}\\] $$ That is, the process can be represented by a system of the following kind: $$ \\[\\begin{cases} Y_1 = \\psi_0 \\varepsilon_1\\\\ Y_2 = \\psi_0\\varepsilon_2 + \\psi_1 \\varepsilon_1 \\\\ Y_3 = \\psi_0\\varepsilon_3 + \\psi_1 \\varepsilon_2 + \\psi_2 \\varepsilon_1 \\\\ \\vdots \\\\ Y_t = \\psi_0\\varepsilon_t + \\psi_1 \\varepsilon_{t-1} + \\psi_2 \\varepsilon_{t-2}+\\dots+ \\psi_{t-1} \\varepsilon_2 + \\psi_{t} \\varepsilon_1 \\\\ \\vdots \\end{cases}\\] $$ ## ARMA A cornerstone of Time series analysis are\\(ARMA\\) models, which are a very general class of models used for forcasting time series, and can be written as \\[ Y_t = \\rho_0 + \\rho_1 Y_{t-1} + \\cdots + \\rho_p Y_{t-p} + \\varepsilon_t + \\theta_{1} \\varepsilon_{t-1} + \\cdots + \\theta_{p} \\varepsilon_{t-q} \\] If we look closely at the above equation, we can see that it has the typical structure of a regression model in which the regressors are of two different kinds: we have lagged versions of the response variable along with lagged values of a white noise shock. Indeed, this is because \\(ARMA\\) models are nothing but the combination of two simpler classes of models. The first is that of Auto-Regressive or \\(AR(p)\\) models, where the response at time \\(\\text{t}\\) is modelled as a linear function of its \\(p\\) previous values. It takes the form: \\[ Y_t = \\rho_0 + \\rho_1 Y_{t-1} + \\cdots + \\rho_p Y_{t-p} + \\varepsilon_t \\] The basic idea behind these models is that if the series exhibits some kind of repeated pattern over time, then looking at its past values might be a good way to extract information on the behaviour of the series and eventually be able to predict new observations. The second class is that of Moving-Average or \\(MA(q)\\) models, where the response at time \\(t\\) is modelled as a linear combination of \\(q\\) lagged versions of a white noise process. These models are of the form: \\[ Y_t = \\mu + \\theta_{1} \\varepsilon_{t-1} + \\cdots + \\theta_{p} \\varepsilon_{t-q} + \\varepsilon_t \\] Here, the main idea is that when the values that a series takes on fluctuate around a constant mean \\((\\mu)\\), a good way to start the prediction is just by using the mean of the series and adjusting it with the information coming from past prediction errors. So, by combining these models we obtain \\(ARMA(p,q)\\) models, where the \\(p\\) and \\(q\\) refer to the number of auto- regressive and moving average components included. We will look at some of these models in further detail. 1.6.1 AR(1) The \\(AR(1)\\) is perhaps the simplest model in Time series analysis. The response is represented as a linear function of its first lag and sometimes an intercept is also included: \\[ Y_t = \\rho_0 + \\rho_1 Y_{t-1} + \\varepsilon_t \\] The first important question we need to ask is under what conditions this process is stationary, and it turns out that it is the case only if \\(|\\rho_1| &lt;1\\). The intuition is that if this were not the case, every new observation of \\(Y\\) would have a value that is further away from the intercept than its previous value. Therefore, the process would not satisfy the first condition that we gave for stationarity, since its expected value would not be constant. Indeed we have: \\[ E[Y_t]=E[\\rho_0 + \\rho_1 Y_{t-1} + \\varepsilon_t]= \\rho_0 + \\rho_1E[Y_{t-1}] \\] noting that \\(Y_{t-1} = \\rho_0 + \\rho_1 Y_{t-2} + \\varepsilon_{t-1}\\) and that it holds for every \\(t\\), we can write $$ \\[\\begin{aligned} E[Y_t] &amp;= \\rho_0 + \\rho_1E[\\rho_0 + \\rho_1 Y_{t-2} + \\varepsilon_{t-1}] = \\rho_0(1+\\rho_1) +\\rho_1^2E[Y_{t-2}] = \\\\ &amp; = \\rho_0(1+\\rho_1 + \\rho_1^2) +\\rho_1^3 E[Y_{t-3}] = \\dots = \\rho_0 \\sum_{t=0}^\\infty \\rho_1^t + \\lim_{t\\to \\infty}\\rho_1^t E[Y_0] \\end{aligned}\\] $$ which converges if and only if \\(|\\rho_1| &lt;1\\). In that case, by the properties of the geometric series, we get \\(E[Y_t]=\\frac{\\rho_0}{1-\\rho_1}\\). We can check by looking at some graphs of simulated \\(AR(1)\\)s that only when \\(|\\rho_1| &lt;1\\) the process is stationary. ## Picture of simulated AR(1) for different values of rho plots = [plot() for i in 1:5];  = 0.0; Y = 0.1; for (i, ) in enumerate((-0.6, 0.1, 0.9, 1, -1.1)) Y = zeros(1001) Y[1] = Y for k in 1:length(Y)-1 Y[k+1] =  + *Y[k] + randn() end label = &quot;rho_1 = $&quot; plot!(plots[i], Y, color=:blue, lw=0.5, marker=:circle,markersize=0.0,alpha=0.6, label=label) plot!(plots[i], legend=:topright, xlabel=&quot;time&quot;, xlim=(0,1005)) plot!(plots[i], seriestype=:hline, [0], linestyle=:dash, alpha=0.7, lw=1, label=&quot;&quot;) end plot(plots..., layout=(5,1), size=(700,900)) It can be shown that the autocovariance function of an \\(AR(1)\\) process is expressed by \\[ \\gamma(h) = \\rho_1^h \\frac{\\sigma^2}{1-\\rho_1^2}, \\quad h=0,1,2,\\dots \\] since this expression depends only on the number of lags \\(h\\), the second condition for weak stationarity is also satisfied. # Picture of the autocovariance function for different values of  plots = [plot() for i in 1:2];  = 1; for (i, ) in enumerate((0.75, -0.75)) times = 0:24 acov = [ * (.^k ./ (1 - .^2)) for k in times] label = &quot;rho = $&quot; plot!(plots[i], times, acov, color=:blue, lw=2, marker=:circle,markersize=3,alpha=0.6, label=label) plot!(plots[i], legend=:topright, xlabel=&quot;Time&quot;, xlim=(0,25)) plot!(plots[i], seriestype=:hline, [0], linestyle=:dash, alpha=0.5, lw=2, label=&quot;&quot;) end plot(plots..., layout=(2,1), size=(700,500), title=&quot;Autocovariance function of an AR(1) process&quot;, titlelocation=:left) We know by the Wolds theorem that when an \\(AR(1)\\) process is stationary, it can be represented as an infinite sum of white noise shocks: \\[ Y_t = \\sum^{\\infty}_{j=0}\\psi^j \\varepsilon_{t-j} \\] Indeed, when \\(|\\rho_1| &lt;1\\), we say that the process is invertible and it can be represented as an \\(MA(\\infty)\\). This can be shown by exploiting the properties of geometric series. Indeed, we can use the lag operator to rewrite $Y_t = Y_{t-1} + _t = L Y_t + _t $. We can rearrange the terms to get \\(Y_t(1-\\rho L) = \\varepsilon_t\\) and so \\[ Y_t = \\frac{\\varepsilon_t}{1-\\rho L} \\] If \\(|\\rho| &lt;1\\), this can be seen as the result of a converging geometric series: \\[ \\frac{\\varepsilon_t}{1-\\rho L}= \\varepsilon_t(1+\\rho L + \\rho^2 L^2 + \\rho^3 L^3 + \\cdots ) \\] Therefore, an AR(1) process can be rewritten as a MA(\\(\\infty\\)): \\[ Y_t = \\sum_{q = 0}^\\infty \\rho^q \\varepsilon_{t-q} = \\varepsilon_t + \\rho \\varepsilon_{t-1}+ \\rho^2 \\varepsilon_{t-2} + \\rho^3 \\varepsilon_{t-3} + \\dots \\] 1.6.2 Estimation and prediction Estimating an \\(AR(1)\\) is pretty straightforward, as it is simple OLS. ## Simulate an AR data generating process  = 0.5;  = 0.7; Y = 0.35; T = 1000; Y = vcat(Y, zeros(T)); for t in 2:length(Y) Y[t] =  + *Y[t-1] + randn() end plot(Y,color=:blue, lw=1, alpha=0.6, xlabel = &quot;Time&quot;, label = &quot;&quot;) ## Estimate the model Y_1 = Y[1:end-1]; # First lag constant = [1 for t in 1:length(Y_1)]; # Intercept X = hcat(constant, Y_1); # Regressors&#39; matrix y = Y[2:end]; # Dependent variable _hat = (X&#39;*X) \\ X&#39;*y # Estimated coefficients ## 2-element Array{Float64,1}: ## 0.48759394482652246 ## 0.6871217732092965 ## Alternative code: X \\ y We see that these values are close to the true ones. We can use these estimated coefficients to forecast future values of \\(Y\\): \\[ \\hat{Y}_{t+1} = \\hat{\\rho}_0 + \\hat{\\rho}_1 Y_{t} \\] ## 1 step ahead prediction Y_hat = _hat[1] + _hat[2]*Y[end] ## 0.5324979007050027 Subsequent values can be forecasted by recursion. We now present a sketch of derivation of the optimal forecast for more steps ahead \\(E[Z_{t+\\ell} | \\ Z_t]\\). Consider for simplicity a centered on the mean version of the model \\(Y_t = \\mu + \\rho_1(Y_{t-1} - \\mu) + \\varepsilon_t\\). By letting \\(Z_t = Y_t - \\mu\\), this can be rewritten as \\(Z_t = \\rho_1 Z_{t-1} + \\varepsilon_t\\). The \\(t+\\ell\\) observation will then be: \\[ \\begin{aligned} Z_{t+\\ell} &amp;= \\rho_1 Z_{t+\\ell-1} + \\varepsilon_t = \\\\ &amp; = \\rho_1(\\rho_1 Z_{t+\\ell-2} + \\varepsilon_{t-1}) + \\varepsilon_{t} = \\rho^2 Z_{t+\\ell-2} + \\rho_1 \\varepsilon_{t-1} + \\varepsilon_t = \\\\ &amp;=\\rho_1^3 Z_{t+\\ell-3} + \\rho_1^2 \\varepsilon_{t-1} + \\rho_1 \\varepsilon_{t-2} + \\varepsilon_t = \\dots \\end{aligned} \\] Since the errors have an expected value of zero (there is not a systematic error in the model), we have that \\(E[Z_{t+\\ell} | \\ Z_t] = \\rho_1^\\ell Z_t\\), that is \\(E[Y_{t+\\ell} | \\ Y_t] = \\mu + \\rho_1^\\ell (Y_t - \\mu)\\). The optimal forecast is therefore \\[ \\hat{Y}_{t+\\ell} = \\hat{\\mu} + \\hat{\\rho}_1^\\ell(Y_t - \\hat{\\mu}) \\] Here a graphical example. ## Plotting l periods ahead forecast ## Forecast ## Training sample l = 12; X_training = X[1:size(X,1)-l-1, :]; y_training = y[1:length(y)-l-1]; ## Estimated coefficients ,  = X_training \\ y_training ## 2-element Array{Float64,1}: ## 0.4903332454530149 ## 0.6878416116324183  = samplemean(y_training) ## 1.5706046797440267 ## Out of sample forecast y_forecast = [ + ^l * (Y[end]-) for l in 1:12]; ## Compare with actual data plot(Y[end-60:end], xlabel = &quot;Time&quot;, label = &quot;Observed values&quot;) plot!(vcat([NaN for i in 1:48], y_forecast), label = &quot;Forecasted values&quot;) 1.6.3 Real world example: GDP series We can now proceed to fit an AR(1) model for the series of GDP growth. Indeed, the PACF plot presented in the first paragraph suggests that including only the first lag might be a good choice. As a first step, we split the series in two parts. Data up to 2018:Q2 are used as a training sample for our model, meaning that we use these data to estimate the coefficients via OLS. The remaining sample (up to 2020:Q2) is used to assess the performances of the model in predicting future values of the GDP growth. ## AR(1) estimation and prediction GDP_1 = GDP[1:end-1]; # First lag constant = [1 for t in 1:length(GDP_1)]; # Intercept X = hcat(constant, GDP_1); # Regressors&#39; matrix y = GDP[2:end]; # Dependent variable_hat = X \\ y # Estimated coefficients _hat = X \\ y ## 2-element Array{Union{Missing, Float64},1}: ## 2.68919146087404 ## 0.12182394815798535 ## 1 step ahead prediction GDP_hat = _hat[1] + _hat[2]*GDP[end] ## 3.5119546156010912 using Dates #Eight periods ahead forecast (2 years) ## Define the training sample dates_training = filter(x -&gt; x &lt;= Date(2018, 4), gdp.data.date); GDP_training = [gdp.data[i, :DY] for i in 2:size(gdp.data,1) if gdp.data[i, :date] in dates_training ]; GDP_outsample = gdp.data[1+length(GDP_training)+1:end, :DY]; # Don&#39;t count the first NaN! T_training = length(GDP_training); T_outsample = length(GDP_outsample); ## Estimation GDP_training1 = GDP_training[1:end-1]; # First lag constant = [1 for t in 1:length(GDP_training1)]; # Intercept X_training = hcat(constant, GDP_training1); y_training = GDP_training[2:end]; ,  = X_training \\ y_training  = samplemean(GDP_training) ## Out of sample forecast y_forecast = [ + ^l * (GDP_training[end]-) for l in 1:T_outsample]; ## Comparison with true data time_period = gdp.data.date[end-4*10:end]; plot(time_period, GDP[end-(4*10):end], xlabel = &quot;Time&quot;, label = &quot;Observed values&quot;); plot!(time_period, vcat([NaN for i in 1:40-T_outsample], y_forecast), label = &quot;Forecasted values&quot;, legend = :bottomleft) 1.6.4 AR(p) As previously noted, when we introduce more lags we speak of \\(AR(p)\\) models. The first question that arises is how many lags should we introduce in our model? Weve indirectly already given an answer to this question. Indeed, a common rule of thumb to decide how many and which lags to include in a model is by looking at the PACF: we should include all those lags that have a statistically different from zero partial autocorrelation. Once the number of lags has been decided the procedures for estimation and prediction are the same as for an \\(AR(1)\\). Another important issue is whether the process is stationary. Unfortunately, answering this question is not as straightforward as with an \\(AR(1)\\). Instead, to see if a model is stationary, we need to look at what are called roots. Note that an \\(AR(p)\\) can be rewritten by using the lag operator as \\[ Y_t = \\rho_1 \\text{L}Y_{t} + \\rho_2\\text{L}^2 Y_{t}+\\cdots + \\rho_p\\text{L}^p Y_{t} + \\epsilon_t \\] from which \\[ (1-\\rho_1L-\\rho_2L^2- \\cdots - \\rho_p L^p)Y_t = \\epsilon_t \\] The process is stationary if all the roots of the polynomial \\((1-\\rho_1L-\\rho_2L^2- \\cdots - \\rho_p L^p)\\) lie outside the unit circle, that is, if theyre all greater than 1 in absolute value. If this is the case the process also admits an \\(MA(\\infty)\\) representation. 1.6.5 MA(1) This process can be represented as \\(Y_t = \\mu + \\phi \\varepsilon_{t-1} + \\varepsilon_t\\), where the process of \\({\\varepsilon_t}\\) is an i.i.d. white noise with mean zero and constant variance \\(\\sigma^2_{\\varepsilon}\\). The process is always stationary as it is a linear combination of white noise processes with constant mean and variance. Its mean is \\(\\mu\\), its variance \\((1 + \\phi_1^2)\\sigma^2\\) and its covariance \\[ \\begin{cases} \\gamma(0) = (1 + \\phi^2)\\sigma^2_\\varepsilon \\\\ \\gamma(1) = \\phi\\sigma^2_\\varepsilon \\\\ \\gamma(h) = 0 \\qquad \\forall h&gt;1 \\end{cases} \\] The basic idea underlying this model is that the stochastic realizations of the process move around an average value. The distance from this value depends on previos periods forecast error. Lets consider an example. Suppose that a firm operating in a relatively stable market has to choose the quantity of production. The firm knows that on average the demand for its products will be \\(\\mu\\), but a stochastic component makes the decision difficult in each period. In the first period, the management decides to produce exactly \\(\\mu.\\) For concreteness, suppose \\(\\mu = 10\\). In the same period, the observed demand is instead \\(Y_1 = 12\\), and so the forecast error is \\(\\varepsilon_1 = \\hat{Y_1} - Y_1 = 10-12 = -2\\), meaning that the firm produced two units less than the demanded quantity. In the following period, the firm adjusts its production by increasing the mean proportionally to the previous periods error: for instance it decides to adjust by half the error,\\(Y_2 = 10 + 0.5 \\cdot (-2)\\), and it does the same for the following periods. An \\(MA(1)\\) model tries to capture this kind of patterns in the data. An \\(MA(1)\\) can also be inverted in an \\(AR(\\infty)\\) in a similar fashion as we did above, again provided that \\(|\\phi|&lt;1\\) . \\[ \\begin{aligned} Y_t &amp;= \\phi \\epsilon_{t-1} + \\epsilon_t \\\\ Y_t &amp;= (1-\\phi L)\\epsilon_t \\\\ \\epsilon_t &amp;= \\frac{Y_t}{1-\\phi L}\\\\ \\epsilon_t &amp;= Y_t(1+\\phi L + \\phi^2 L^2 + \\cdots + \\phi^\\infty L^\\infty) \\\\ Y_t &amp;= - \\phi Y_{t-1} - \\phi^2 Y_{t-2} - \\cdots - \\phi^\\infty Y_{t-\\infty} + \\epsilon_t \\\\ Y_t &amp;= -\\sum^\\infty_{j=1}\\phi^jY_{t-j} + \\epsilon_t \\end{aligned} \\] When presenting \\(AR\\) models we saw that the PACF plot gives a rule of thumb to decide how many lags to include in the model. In the same way, we can use the ACF plot to decide how many lags of the white noise shock to include. For example, if we were to forecast GDP using an \\(MA\\) model, by looking at the graph presented in the first section we would pick the first two lags. "],["seasonal-adjustment.html", "Topic 2 Seasonal Adjustment 2.1 The components of a time series 2.2 The causes of seasonality 2.3 Why to adjust for seasonality 2.4 Decomposition models 2.5 Official programs for seasonal adjustment 2.6 Illustrative example 2.7 RJDemetra package 2.8 Preprocessing 2.9 Decomposition 2.10 Additional output and forecast", " Topic 2 Seasonal Adjustment Many socio-economic time series present seasonal patterns. Agricultural production is inherently related with seasonal climatic conditions, the sales of many goods are higher during the Christmas period, air traffic is heavier during the summer We can easily think of many such real-world examples. Indeed, seasonal variations are a main component in many observed time series and they depend on the relative importance that a specific month or quarter has for an activity. Of course, these variations cannot be observed in time series collected at an annual frequency (for which there are no observations in different periods of the year), so that seasonal adjustment is needed only for data with more than one observation per year. There are other sources of seasonality that can be harder to handle, for example moving-holidays and trading days variations. The moving-holiday component depends on holidays that are not always on the same day of a month and that vary according to each countrys religion and tradition. It has a strong impact on many variables: consider, for instance, the level of sales, car and air traffic, gasoline sales, etc. during Easter, the Chineese New Year, or the effect that Ramadan has on the food industry in Islamic countries. On the other hand, trading-days variations refer to the number of working days and weekends in the considered period. Many economic events occur with different frequency or intensity during Saturday or Sunday, making it important to correct the series for the number of weekends in the given month or quarter. Indeed, when we are given the observations of two different periods, this procedure allows distinguishing between the variations due merely to the presence of more weekends and the differences testifying deeper trends. 2.1 The components of a time series A time series can be considered as made up by one or more of the following components: a trend component, which is the long term dynamics of the time series; a cycle components, which accounts for a repeating pattern in the stochastic process and is typical of macroeconomic time series such as GDP; a seasonal component, which gathers the fluctuations that repeat with a certain degree of regularity from year to year; a calendar-effect component, which is related to the working-days variations (one more Saturday or Sunday in a month could have some effects: consider for instance the higher consumption of gasoline during the week-end) and to the moving-holidays; an irregular component, formed by the residuals and that does not depend on other systematic components; outliers, which need to be specifically treated when estimating the models and are not removed by the procedure of seasonal adjustment. Outliers can be classified into three categories: Additive Outlier (AO): a single value outside the expected range of the time series. It can be caused by random effects or by an identifiable event, such as a strike or a flood. Temporary Change (TC): after an extremely high or low value is observed, it takes some time for the series to come back to its initial level. The Covid-19 pandemic could be considered as such. Level Shift (LS): a sudden and permanent change in the level of the time series after a given period. It often depends on changes in the definition of the survey population and in the collection method. 2.2 The causes of seasonality Seasonality can arise for different reasons, as in the examples above. It can be caused by climatic cycles, as the differences in temperature and weather in different seasons, or by institutions, namely the social habits, traditional festivals and also administrative rules and deadlines. 2.3 Why to adjust for seasonality The main reason for seasonal adjustment is to distinguish between the seasonal component and the other sources of variability in the original data. Indeed, seasonally-adjusted data reflect the variations due only to the other components. This allows for a more consisten comparation across time and space. For instance, we can use adjusted data to compare the industrial production in March with that of the Christmas holidays, or to compare countries with different climates or vacations. Moreover, the seasonally adjusted series gives far more perspicuous information about current economic conditions, which is essential to business cycle analysis. For this reason, it is the ground for forecasting and decision making. 2.4 Decomposition models We can think the different components of a time series to build the series in different ways. The additive model consider the series as the sum of the components: \\[ Y_t = T_t + C_t + S_t + I_t \\] The multiplicative mode, instead, considers the product of the components: \\[ Y_t = T_t \\cdot C_t \\cdot S_t \\cdot I_t \\] While the additive models are a good choice for series that (even if not trend-stationary) present changing variance, the multiplicative models are better suited to deal with time series whose variance changes over time. 2.5 Official programs for seasonal adjustment Seasonal adjustment is usually performed by official statistics institutions, which publish already-deseasonalized time series. There are two standard software and procedures for seasonal adjustment: X-13-ARIMA, developed by the Census Bureau. It uses a filter-based, non-parametric approach to estimate the seasonal component. TRAMO-SEATS, developed by the Bank of Spain.It uses a model-based, parametric method to decompose the time series. The software JDemetra+, developed by Eurostat, easily allows to perform the seasonal adjustment with both the X13-ARIMA and TRAMO-SEATS methods. In what follows, we will show how to use the R interface to JDemetra+ provided by the RJDemetra package. Before doing that, we will present a simple example giving an intuition of the procedure. 2.6 Illustrative example We will now display a simple procedure to seasonally adjust a time series. Consider the following time series (which we will denote as time_series), collecting observations over four years, measured with a quarter-year frequency. If we plot it we can see that it displays a clear recurring pattern. time_series &lt;- c(19,21,34,9,15,18,39,11,19,22,31,13,19,18,31,11) T_ts &lt;- length(time_series) plot(time_series, type=&quot;b&quot;, xaxt=&quot;n&quot;, xlab = &quot;Quarter&quot;) axis(1, at = 1:T_ts, labels=rep(paste0(&quot;Q&quot;, 1:4), T_ts/4), cex.axis=0.7) As a first step, we collect the data into a table. As already highlighted by the plot, we can note that, in every year, the highest observation is the one corresponding to the third quarter, and the lowest corresponds to the fourth quarter. tble &lt;- time_series dim(tble)=c(4,4) tble &lt;- t(tble) rownames(tble) &lt;- 1990:1993 colnames(tble) &lt;- paste0(&quot;Q&quot;, 1:4) tble ## Q1 Q2 Q3 Q4 ## 1990 19 21 34 9 ## 1991 15 18 39 11 ## 1992 19 22 31 13 ## 1993 19 18 31 11 We can then compute the average value of the series for every year: average=rowMeans(tble) tble_1=cbind(tble, average) tble_1 ## Q1 Q2 Q3 Q4 average ## 1990 19 21 34 9 20.75 ## 1991 15 18 39 11 20.75 ## 1992 19 22 31 13 21.25 ## 1993 19 18 31 11 19.75 We then divide each value by the average of its corresponding year. In doing so, we obtain a table collecting all the percentage deviations from each years average. tble_2 &lt;- tble_1[,1:4]/average tble_2 ## Q1 Q2 Q3 Q4 ## 1990 0.9156627 1.0120482 1.638554 0.4337349 ## 1991 0.7228916 0.8674699 1.879518 0.5301205 ## 1992 0.8941176 1.0352941 1.458824 0.6117647 ## 1993 0.9620253 0.9113924 1.569620 0.5569620 We now average over each quarter to get what are called Seasonal Indices. These express, for each quarter, how much the corresponding observations tend to deviate from the average. They are therefore a measure of the seasonal effect present in the series. SIndex &lt;- colMeans(tble_2) tble_3 &lt;- rbind(tble_2, SIndex) tble_3 ## Q1 Q2 Q3 Q4 ## 1990 0.9156627 1.0120482 1.638554 0.4337349 ## 1991 0.7228916 0.8674699 1.879518 0.5301205 ## 1992 0.8941176 1.0352941 1.458824 0.6117647 ## 1993 0.9620253 0.9113924 1.569620 0.5569620 ## SIndex 0.8736743 0.9565511 1.636629 0.5331455 Interestingly, the sum of the indices is always equal to the number of seasons (in our case of quarters) present. sum(SIndex) ## [1] 4 Finally, to deseasonalize the original series (that is to remove the seasonal effect as summarized by the seasonal indices), we divide each observation by the corresponding seasonal indicex according to the formula: \\[ \\text{Deseasonalized} = \\frac{\\text{Observed}}{\\text{Seasonal Index}} \\] tble_4 &lt;- tble/SIndex[col(tble)] tble_4 ## Q1 Q2 Q3 Q4 ## 1990 21.74723 21.95387 20.77441 16.88094 ## 1991 17.16887 18.81760 23.82947 20.63226 ## 1992 21.74723 22.99929 18.94137 24.38359 ## 1993 21.74723 18.81760 18.94137 20.63226 We can finally plot the resulting series: result=as.vector(t(tble_4)) plot(result, type=&quot;b&quot;, ylim=c(10,40), xlab=&quot;Quarter&quot;, ylab=&quot;Deseasonalized time_series&quot;, xaxt = &quot;n&quot;) axis(1, at = 1:T_ts, labels=rep(paste0(&quot;Q&quot;, 1:4), T_ts/4), cex.axis=0.7) We can appreciate how the resulting time series is way smoother than the original and, more importantly, no longer exhibits a clear seasonal pattern. The actual algorithms and models employed to perform seasonal adjustment are more sophisticated, but the previous example can still gives good insight of what seasonal adjustment is about. 2.7 RJDemetra package Remark: the RJDemetra package requires the preliminary installation of Java SE 8 (or later versions) and the R pakcage rJava. #install.packages(&quot;rJava&quot;) #install.packages(&quot;RJDemetra&quot;) library(RJDemetra) To illustrate the functioning of the package we will usa the dataset embedded in it, ipi_c_eu, which collects monthly industrial production indices in manifacturing in the EU. The baseline year is 2015 and the series are unadjusted. When using your own data, you shall transform them in a Time-Series object. Here we provide an example: example_data &lt;- c(1943.1,1951.53,1983.23,1927.27,2054.57,2056.27,2019.18,2072.2,2069.99,2082.86,2111.94,2099.29,2094.14,2039.87,1944.41,2024.81,2080.62,2054.08,1918.6,1904.42,2021.95,2075.54,2065.55,2083.89,2148.9,2170.95,2157.69,2143.02,2164.99,2246.63,2275.12,2329.91,2366.82,2359.31,2395.35,2433.99) example_ts &lt;- ts(example_data, start = c(2010,1), frequency = 12) example_ts ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct ## 2010 1943.10 1951.53 1983.23 1927.27 2054.57 2056.27 2019.18 2072.20 2069.99 2082.86 ## 2011 2094.14 2039.87 1944.41 2024.81 2080.62 2054.08 1918.60 1904.42 2021.95 2075.54 ## 2012 2148.90 2170.95 2157.69 2143.02 2164.99 2246.63 2275.12 2329.91 2366.82 2359.31 ## Nov Dec ## 2010 2111.94 2099.29 ## 2011 2065.55 2083.89 ## 2012 2395.35 2433.99 We take as an example the data for France from ipi_c_eu: ipi_fr &lt;- ipi_c_eu[,&quot;FR&quot;] plot(ipi_fr) As explained in the first section, the two most common procedures are the X-13ARIMA-SEATS method and the TRAMO-SEATS method. They both consist of two linked parts: the preprocessing step and the decomposition step. The first is common to both procedures and is based on fitting a RegARIMA model to estimate the effect of trading day variations and outliers and remove them from the original series. 2.8 Preprocessing The RegARIMA model (model with ARIMA errors) is specified as \\[ z_t=y_t \\cdot \\beta+x_t \\] where  \\(z_t\\) is the original series;  \\( = (_1, ..., _n)\\) is a vector of regression coefficients;  \\(y_t = (y_{1t}, ..., y_{nt})\\) are \\(n\\) regression variables (outliers, calendar effects, user-defined variables);  \\(x_t\\) is a disturbance that follows the general ARIMA process. Note that the automatically identified specification of the RegARIMA model might differ between X-13 and TRAMO-SEATS (meaning that the number of RA or MA components can be different) but the fitting procedure is the same. x13(ipi_fr)$regarima ## y = regression model + arima (2, 1, 1, 0, 1, 1) ## Log-transformation: no ## Coefficients: ## Estimate Std. Error ## Phi(1) 0.0003269 0.108 ## Phi(2) 0.1688192 0.074 ## Theta(1) -0.5485606 0.102 ## BTheta(1) -0.6660849 0.042 ## ## Estimate Std. Error ## Monday 0.55932 0.228 ## Tuesday 0.88221 0.228 ## Wednesday 1.03996 0.229 ## Thursday 0.04943 0.229 ## Friday 0.91132 0.230 ## Saturday -1.57769 0.228 ## Leap year 2.15403 0.705 ## Easter [1] -2.37950 0.454 ## TC (4-2020) -35.59245 2.173 ## AO (3-2020) -20.89026 2.180 ## AO (5-2011) 13.49850 1.857 ## LS (11-2008) -12.54901 1.636 ## ## ## Residual standard error: 2.218 on 342 degrees of freedom ## Log likelihood = -799.1, aic = 1632 aicc = 1634, bic(corrected for length) = 1.855 tramoseats(ipi_fr)$regarima ## y = regression model + arima (2, 1, 0, 0, 1, 1) ## Log-transformation: no ## Coefficients: ## Estimate Std. Error ## Phi(1) 0.4032 0.051 ## Phi(2) 0.2883 0.051 ## BTheta(1) -0.6641 0.042 ## ## Estimate Std. Error ## Week days 0.6994 0.032 ## Leap year 2.3231 0.690 ## Easter [6] -2.5154 0.436 ## AO (5-2011) 13.4679 1.787 ## TC (4-2020) -22.2128 2.205 ## TC (3-2020) -21.0391 2.217 ## AO (5-2000) 6.7386 1.794 ## ## ## Residual standard error: 2.326 on 348 degrees of freedom ## Log likelihood = -816.1, aic = 1654 aicc = 1655, bic(corrected for length) = 1.852 As we can see, the functions x13 and tramoseats automatically fit the best RegARIMA model for the given time series. For the regressive part (\\(y_t\\)), the functions include several dummy variables to check for the effects of the weekdays (Monday-Saturday) and outliers. The outliers are reported in parentheses and classified according to their type (Additive Outliers, Transitory Change, Level Shift). The ARIMA model is automatically selected to best fit the data, by choosing the number of autoregressive and moving average components (both regular and seasonal). Regular components are constrained to be at most 3, while seasonal components can be at most 1 per type. The frequency of observations per year (that is, the number of observations per year) is automatically detected. The output reports the estimated coefficients and the standard errors. 2.9 Decomposition The second part instead differs across the two procedures. X-13 is based on a non-parametric filter approach and exploits the X-11 algorithm, while TRAMO-SEATS is parametric. The main idea here is that a time series, depending on its characteristics, can be decomposed into trend, cycle, seasonal component and residual. That is: \\[ x_t = T_t + C_t + S_t + I_t \\] An alternative model is the multiplicative one, which is used when the variance of the time series is not constant but increases over time: \\[ x_t = T_t\\cdot C_t \\cdot S_t \\cdot I_t \\] or \\[ x_t = T_t\\cdot (1 + C_t) \\cdot (1 + S_t) \\cdot (1 + I_t) \\] For this example we use the X-13 procedure. x13_model &lt;- x13(ipi_fr) We begin by plotting the series, and noting right away that the seasonal component is very strong: every year, during summer, industrial production in France is notably lower than during the rest of the year ## Original series plot(x13_model$final$series[,1]) Next, the trend is recovered by means of an n-order polynomial smoothing procedure. This can be thought of as a simple approximation of the series shape. ## Trend of the series plot(x13_model$final$series[,3]) By subtracting the trend to the original series we are left with the seasonal component: ## Seasonal component plot(x13_model$final$series[,4]) At this stage, the seasonal components are averaged across every year to get the Seasonal Indices (one per month in our case). ## Seasonal indices seasonal_index &lt;- colMeans(matrix(x13_model$final$series[,4], nrow = 30, ncol = 12, byrow = T)) ## Warning in matrix(x13_model$final$series[, 4], nrow = 30, ncol = 12, byrow = T): data ## length [372] is not a sub-multiple or multiple of the number of rows [30] plot(seasonal_index, type = &quot;l&quot;) which repeated for every year gives ## Overall period seasonal_index_repeated &lt;- rep(seasonal_index, times = 30) plot(seasonal_index_repeated, type = &quot;l&quot;) By subtracting the last series to the original data we obtain the seasonally adjusted series (that is the series where the seasonal component has been removed). ## Seasonally adjusted series plot(x13_model$final$series[,2]) Finally, this resulting series is newly approximated with the polynomial smoothing procedure and the difference between the seasonally adjusted series and this last result gives the residuals: ## Residuals plot(x13_model$final$series[,5]) 2.10 Additional output and forecast M and Q statistics The X-13 function also provides the M-statistic and the Q-statistic. These can be found under the decomposition section of the output: x13_model[2] ## $decomposition ## [1m Monitoring and Quality Assessment Statistics: [22m ## M stats ## M(1) 0.163 ## M(2) 0.089 ## M(3) 1.181 ## M(4) 0.558 ## M(5) 1.020 ## M(6) 0.090 ## M(7) 0.083 ## M(8) 0.244 ## M(9) 0.062 ## M(10) 0.272 ## M(11) 0.256 ## Q 0.368 ## Q-M2 0.402 ## ## Final filters: ## Seasonal filter: 3x5 ## Trend filter: 13 terms Henderson moving average The M-statistics are a set of 11 statistics that check the quality of the seasonal adjustment. Their value varies between 0 and 3 but only values smaller than 1 are considered acceptable. The statistics from M1 to M6 check the characteristics of the irregular component; M7 the presence of seasonality; M8 to M11 the stability of the seasonal component. The Q-statistics are linear combinations of the M-statistics that summarize in a single number all theis information on the decomposition. Filters Under the decomposition section its also possible to find the specification of the filters that have been used in the estimation of the different factors. These include seasonal filters and trend filters. The main idea of a filter is that of smoothing the observations of a time series. This is usually done by replacing each original value with linear combinations of its adjacent values. Trend filters smooth the original series to isolate stable trends in the data. For example, a 2x4 filter can be outlined as follows: $$ \\[\\begin{aligned} 2002.1 &amp;+&amp; 2002.2 &amp;+&amp; 2002.3 &amp;+&amp; 2003.4 \\\\ &amp;&amp; 2002.2 &amp;+&amp; 2002.3 &amp;+&amp; 2002.4 &amp;+&amp; 2003.1\\\\ \\hline &amp;&amp; &amp;&amp; 8 &amp;&amp; &amp;&amp; \\end{aligned}\\] $$ 2x4 in the above example refers to the number of sliding windows (2) and the number of observations in each window (4). Seasonal filters are instead applied only to observations pertaining to the same season. For example, a 3x5 seasonal filter works as follows: \\[ \\begin{aligned} 2002.1 &amp;+&amp; 2003.1 &amp;+&amp; 2004.1 &amp;+&amp; 2005.1 &amp;+&amp; 2006.1 \\\\ &amp;&amp; 2003.1 &amp;+&amp; 2004.1 &amp;+&amp; 2005.1 &amp;+&amp; 2006.1 &amp;+&amp; 2007.1 \\\\ &amp;&amp; &amp;&amp; 2004.1 &amp;+&amp; 2005.1 &amp;+&amp; 2006.1 &amp;+&amp; 2007.1 &amp;+&amp; 2008.1 \\\\ \\hline &amp;&amp; &amp;&amp; &amp;&amp; 15 &amp;&amp; &amp;&amp; \\end{aligned} \\] Here, the value of observation for the first quarter of 2005 is replaced with a linear combination of the previous and subsequent values of the series during the same season. Forecast The Final section of the X-13 functions output lists the final part of the original series, its adjusted version and explicits the different components contribution (trend, seasonal, irregular) to each observation of the time series. x13_model[3] ## $final ## Last observed values ## y sa t s i ## Jan 2020 101.0 102.95613 102.9586 -1.95613209 -0.002494203 ## Feb 2020 100.1 103.50876 102.9592 -3.40875640 0.549602816 ## Mar 2020 91.8 82.87617 103.1664 8.92382800 -20.290271773 ## Apr 2020 66.7 66.65243 103.5971 0.04756625 -36.944710027 ## May 2020 73.7 78.87836 104.0393 -5.17835604 -25.160905985 ## Jun 2020 98.2 87.34544 104.3804 10.85456021 -17.034985133 ## Jul 2020 97.4 92.47436 104.5319 4.92563707 -12.057551871 ## Aug 2020 71.7 97.47245 104.3751 -25.77244698 -6.902636199 ## Sep 2020 104.7 97.37717 103.9182 7.32282919 -6.541070626 ## Oct 2020 106.7 98.24194 103.3047 8.45805540 -5.062719500 ## Nov 2020 101.6 100.26862 102.7746 1.33138152 -2.506014899 ## Dec 2020 96.6 99.66730 102.5133 -3.06729670 -2.845961796 ## ## Forecasts: ## y_f sa_f t_f s_f i_f ## Jan 2021 94.53021 101.0902 102.4794 -6.56002888 -1.3891608 ## Feb 2021 97.90024 101.7395 102.5246 -3.83928384 -0.7850772 ## Mar 2021 114.09983 102.3065 102.5087 11.79328397 -0.2021598 ## Apr 2021 102.16781 102.2220 102.3759 -0.05422341 -0.1538967 ## May 2021 96.01612 101.5450 102.2100 -5.52888123 -0.6650098 ## Jun 2021 112.76658 101.3438 102.0725 11.42275939 -0.7286526 ## Jul 2021 104.13805 101.6681 102.0297 2.46989932 -0.3615193 ## Aug 2021 79.13003 102.3617 102.1360 -23.23171595 0.2257112 ## Sep 2021 109.06438 102.4772 102.3249 6.58713572 0.1523168 ## Oct 2021 108.64207 102.1329 102.5185 6.50921416 -0.3856588 ## Nov 2021 106.46022 102.5908 102.6996 3.86943752 -0.1088338 ## Dec 2021 99.79901 103.0831 102.8580 -3.28410831 0.2251086 This section also provides forecast for a period after the last observations. This is based on the ARIMA model. Diagnostics The final part of the output of the X-13 function provides some additional statistics such as the Kruskall-Wallis test, the test for the presence of seasonality assuming stability, the Evolutive Seasonality test and Residual seasonality tests. "],["temporal-aggregation-and-disaggregation.html", "Topic 3 Temporal aggregation and disaggregation 3.1 Aggregation 3.2 Disaggregation 3.3 An example using Eurostat data 3.4 References", " Topic 3 Temporal aggregation and disaggregation Very often, time series data that is required in empirical analyses consists of variables at different frequencies. Unless one is willing to use specific mixed-frequencies models, it is usually necessary to convert some of the series so that all the data has the same frequency. In what follows, we will present the techniques for both reducing and increasing the frequency of a time series: these techniques are, respectively, temporal aggregation and temporal disaggregation. We will complete the theoretical exposition with some examples using macroeconomic data from the Eurostats national account database. In particular, we will show the usage of the tempdisagg library, which offers some useful functions for both temporal aggregation and disaggregation. #install.packages(&quot;tempdisagg&quot;) #install.packages(&quot;eurostat&quot;) #install.packages(&quot;knitr&quot;) library(tempdisagg) library(eurostat) library(knitr) gdp.q &lt;- read.csv(&quot;gdp.q.csv&quot;) gdp.q &lt;- ts(gdp.q$values, start = c(1995,1), end = c(2019, 4), frequency = 4) gdp.a &lt;- read.csv(&quot;gdp.a.csv&quot;) gdp.a &lt;- ts(gdp.a$values, start = c(1995,1), end = c(2019, 1), frequency = 1) exports.q &lt;- read.csv(&quot;exports.q.csv&quot;) exports.q &lt;- ts(exports.q$values, start = c(1995,1), end = c(2019, 4), frequency = 4) imports.q &lt;- read.csv(&quot;imports.q.csv&quot;) imports.q &lt;- ts(imports.q$values, start = c(1995,1), end = c(2019, 4), frequency = 4) employment.a &lt;- read.csv(&quot;employment.a.csv&quot;) employment.a &lt;- ts(employment.a$values, start = c(1995,1), end = c(2019, 1), frequency = 1) employment.q &lt;- read.csv(&quot;employment.q.csv&quot;) employment.q &lt;- ts(employment.q$values, start = c(1995,1), end = c(2019, 4), frequency = 4) 3.1 Aggregation Time aggregation consists of manipulating a series in order to make it available at a lower frequency. For instance, we could need to retrieve annualized GDP given the quarterly data, or the weekly price change of a stock for which data at a daily frequency is available. Of course, aggregating a time series reduces the number of observations, which results in the loss of some information. A large literature has studied the effects of aggregation on forecast performances and on the statistical properties of time series models, showing that the impact can be large. Intuitively, since the information is condensed in a lower number of observations, aggregation reduces the variability of the data, at the cost of part of this information being lost. In practice, we usually take the average value of different observations, which tends to smooth the time series: even if smoothing can be desirable in some contexts (e.g.Â for seasonal adjustment), it deletes the variation at the high-frequency level, and it can therefore preclude the model from detecting regularities at this level. As a consequence, the resulting model will produce predictions that are more stable and less volatile than those that would be obtained using disaggregated data: this is not good news, since it means that we are missing part of the story that the data contains. Temporal aggregation of a time series is a quite straightforward operation, which requires to keep in mind only a few caveats regarding the nature of the variable being temporally aggregated. Indeed, there are four main techniques of temporal aggregation which should be used depending on whether we are considering stock or flow variables. All aggregation methods ensure that either the sum, the average, the first or the last value of the resulting low-frequency series is consistent with the high-frequency series. Flow variables. Consider, for instance, the case of quarterly GDP. If we want to retrieve annual GDP, what we have to do is summing each years quarterly values to get the total amount produced in a given country each year. This is how we usually deal with flow variables: the total low-frequency flow is the sum of all its high-frequency flows. In our example, 2020 GDP of a given country will be the sum of 2020:Q1, , 2020:Q4 GDP: \\[ \\text{GDP}_{2020} = \\text{GDP}_{2020:Q1} + \\text{GDP}_{2020:Q2} + \\text{GDP}_{2020:Q3} + \\text{GDP}_{2020:Q4} \\] Stock variables. Consider the example of the total stock of capital in a country. Of course, summing all the values of the total capital of each quarter would make no sense, as we would end up counting four times the same capital stock. Temporal aggregation, in this case, can be performed in three ways, depending on the nature of the variable considered and on the aim of the empirical researcher: Taking the average of the high-frequency observations. For instance, if we want to consider the annual stock of capital as the average level of its quarterly values, the annual capital in 2020 will be: \\[ \\text{Capital}_{2020} = \\frac{\\text{Capital}_{2020:Q1} + \\text{Capital}_{2020:Q2} + \\text{Capital}_{2020:Q3} + \\text{Capital}_{2020:Q4}}{4} \\] Taking only the last observation of the high-frequency variable. For instance, we could consider as the most reliable measure of the total capital stock the last data collected by Eurostat: \\[ \\text{Capital}_{2020} = \\text{Capital}_{2020:Q4} \\] Taking only the first observation of the high-frequency variable. On the contrary, for our study, we could be interested in considering as the annual capital stock its value at the beginning of the year: \\[ \\text{Capital}_{2020} = \\text{Capital}_{2020:Q1} \\] We report below an example using Eurostat data for the GDP of the United Kingdom. ## True annual GDP plot(gdp.a) ## Annualized GDP gdp.annualized &lt;- ta(gdp.q, conversion = &quot;sum&quot;, to = &quot;annual&quot;) plot(gdp.annualized) ## True annual total employment (in thousands) plot(employment.a) ## Annualized total employment employment.annualized &lt;- ta(employment.q, conversion = &quot;average&quot;, to = &quot;annual&quot;) plot(employment.annualized) 3.2 Disaggregation Temporal disaggregation refers to the process of transforming a low-frequency time series (for example annual) into a higher frequency series (for example quarterly or monthly). Temporal disaggregation can be performed with or without one or more high-frequency indicator series, useful to inform the process. The most common procedures to perform disaggregation are Denton and Denton-Cholette, which are concerned with movement preservation and generate a series similar to the indicator but can also be performed without it, and Chow-Lin, Litterman and Fernandez, which are regression-based methods that always use one or more indicator series. The Chow-Lin procedure is best for stationary or cointegrated series, while Litterman and Fernandez non-cointegrated series. All of these procedures can be presented in a general framework. Suppose that you observe a low frequency series, denoted by \\(y_l\\), and that you want to retrieve the higher frequency variable \\(y\\), which is unobserved. The first step to do so is to tale an indicator series, denoted by \\(x\\), which is available at quarterly frequency. Of course a good indicator series is one that moves together with the unobserved series \\(y\\). The next step is to perform the aggregation of the indicator series so that it matches the freqency of \\(y_l\\). This is performed by means of the conversion matrix \\(C\\): \\[ x_l = Cx, \\] All disaggregation methods ensure that either the sum, the average, the first or the last value of the resulting high frequency series is consistent with the low frequency series, and this aspect is controlled by the matrix \\(C\\). For example, if \\(x\\) is at a quarterly frequency and we want to perform annualization so that the sum is consistent with \\(y_l\\), we will use a matrix of the form: \\[ C = I_{n_l} \\otimes [1,1,1,1] = \\underset{n_l \\times n}{ \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; \\cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ &amp; \\vdots &amp;&amp;&amp; &amp;&amp;&amp;&amp; &amp;&amp;&amp; \\vdots &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\end{bmatrix}} \\] If instead we require average-consistency, \\(C\\) will be of the form: \\[ C = I_{n_l} \\otimes [0.25,0.25,0.25,0.25] = \\underset{n_l \\times n}{ \\begin{bmatrix} 0.25 &amp; 0.25 &amp; 0.25 &amp; 0.25 &amp; \\cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ &amp; \\vdots &amp;&amp;&amp; \\ddots &amp;&amp;&amp; \\vdots &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0.25 &amp; 0.25 &amp; 0.25 &amp; 0.25 \\end{bmatrix}} \\] You then subtract \\(x_l\\) (pointwise) from \\(y_l\\), obtaining the vector of residuals \\(u_l\\). Finally, is estimated according to: \\[ \\hat{y} = x + Dx_l , \\] where \\(D\\) is a distribution matrix. The different disaggregation methods differ for the assumptions they rely on and on how the \\(D\\) matrix are constructed. They can also differ in the number of indicator series employed. 3.2.1 Denton and Denton-Cholette Indicator series - one or no indicator series. \\(x\\) is simply a vector, using a vector of ones (a constant), is equivalent to having no indicator series. Distribution matrix - minimizes the squared absolute (or relative) deviations from a (differenced) indicator series, where the parameter h defines the degree of differencing. For the additive Denton methods and for \\(h = 0\\), the sum of the squared absolute deviations between the indicator and the final series is minimized. For \\(h = 1\\), the deviations of first differences are minimized, for \\(h = 2\\), the deviations of the differences of the first differences, and so on. For the proportional Denton methods, deviations are measured in relative terms. The Denton-Cholette method is generally preferable and differs from the simple Cholette in that it removes the spurious transient movement at the beginning of the resulting series. In this case the distribution matrix is indicated with \\(D_{DC}\\). 3.2.2 Chow-Lin, Fernandez, Litterman These are regression based methods that perform GLS of \\(y_l\\) on the annualized indicator series \\(x_l = Cx\\) . Note that in this case \\(x\\) can be a matrix since more than one indicator series can be employed. The critical assumption of these methods is that the linear relationship between the annualized indicator series and \\(y_l\\) also holds between the quarterly indicator series and \\(y\\). Indicator series - one or more indicator series. In this case the vector \\(u_l\\) is obtained by subtracting a transformed version of the indicator series: \\[ u_l = y_l - C\\widetilde{x} \\] where \\(\\widetilde{x}=\\hat{\\beta}x\\) . \\(\\hat{\\beta}\\) is the regression coefficient obtained by regressing \\(y_l\\) on the aggregated version of the indicator series, \\(x_l = Cx\\). For a given variance-covariance matrix, \\(\\Sigma\\), the GLS estimator, \\(\\hat{\\beta}\\) , is calculated in the standard way: \\[ \\hat{\\beta}(\\Sigma) = [x&#39;C&#39;(C\\Sigma C&#39;)^{-1}Cx]^{-1}x&#39;C&#39;((C\\Sigma C&#39;)^{-1})y_l \\] * Distribution matrix - these methods use \\(D = \\Sigma&#39;C&#39;(C\\Sigma C&#39;)^{-1}\\) as distribution matrix. The 3 methods differ in how the \\(\\Sigma\\) matrix is calculated. Chow-Lin assumes that the quarterly residuals follow an autoregressive process of order 1 (AR1) where the innovations are \\(WN(0, \\sigma_e)\\) and the autoregressive parameter \\(\\rho &lt; 1\\). The resulting covariance matrix has the following form: \\[ \\Sigma_{CL}(\\rho) = \\frac{\\sigma^2_{\\epsilon}}{1-\\rho^2} \\begin{bmatrix} 1 &amp; \\rho &amp; \\cdots \\rho^{n-1} \\\\ \\rho &amp; 1 &amp; \\cdots &amp; \\rho^{n-2} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ \\rho^{n-1} &amp; \\rho^{n-2} &amp; \\cdots &amp; 1 \\end{bmatrix} \\] Fernandez and Litterman deal with cases when the quarterly indicators and the annual series are not cointegrated. They assume that the quarterly residuals follow a nonstationary process (Fernandez is a special case of Litterman, where \\(\\rho = 0\\)). The resulting covariance matrix has the following form: \\[ \\Sigma_{L}(\\rho) = \\sigma^2_{\\epsilon} \\left(\\Delta&#39;H(\\rho)&#39;H(\\rho) \\Delta\\right)^{-1} \\] where \\[ \\Delta = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ -1 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; -1 &amp; 1 &amp;\\cdots &amp; 0 \\\\ &amp; \\vdots &amp; &amp; &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 1 \\\\ \\end{bmatrix} \\] and \\[ H(\\rho) = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ -\\rho &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; -\\rho &amp; 1 &amp;\\cdots &amp; 0 \\\\ &amp; \\vdots &amp; &amp; &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 1 \\\\ \\end{bmatrix} \\] The autoregressive parameter \\(\\rho\\) in the Chow-Lin and Litterman methods can be estimated in several different ways. An introductory exposition can be found in the references. 3.3 An example using Eurostat data In the following example we use Eurostat data for UK, we take annual GDP as \\(y_l\\) and we use quarterly exports and imports as indicator series to disaggregate GDP and obtain \\(\\hat{y}\\). Of course we dispose also of the true \\(y\\), quarterly GDP, which we use to evaluate our results. We first retrieve the data with the eurostat package: ## Obtain the GDP data from Eurostat query &lt;- search_eurostat(pattern = &quot;GDP&quot;, type = &quot;dataset&quot;, fixed = F) id.a &lt;- query$code[1] id.q &lt;- query$code[2] ## True quarterly data gdp.q_raw &lt;- get_eurostat(id.q, #type = &quot;label&quot;, #select_time = &quot;Y&quot;, time_format = &quot;date&quot;, # alternative, &quot;num&quot; filters = list(geo = &quot;UK&quot;, ## Germany (until 1990 former territory of the FRG) na_item = &quot;B1GQ&quot;, ## Gross domestic product at market prices unit = &quot;CP_MEUR&quot;, ## Euro series (CP MEUR ) are derived from #transmitted national currency series using historic exchange rates. #They are suitable for internal comparison and aggregation. #When comparing them over time, account must be taken of exchange rate effects. s_adj = &quot;NSA&quot;)) ## Not seasonal nor calendar adjustment label_eurostat_vars(gdp.q_raw) ## Variables gdp.q &lt;- gdp.q_raw[, c(&quot;time&quot;, &quot;values&quot;)] kable(head(gdp.q)) plot(gdp.q, type = &quot;l&quot;) ## Annual data gdp.a_raw &lt;- get_eurostat(id.a, time_format = &quot;date&quot;, filters = list(geo = &quot;UK&quot;, na_item = &quot;B1GQ&quot;, unit = &quot;CP_MEUR&quot;, s_adj = &quot;NSA&quot;)) gdp.a &lt;- gdp.a_raw[, c(&quot;time&quot;, &quot;values&quot;)] kable(head(gdp.a)) plot(gdp.a, type = &quot;l&quot;) ## Indicator series: industrial production query &lt;- search_eurostat(pattern = &quot;industr&quot;, type = &quot;dataset&quot;, fixed = T) query$title exports.q_raw &lt;- get_eurostat(&quot;namq_10_exi&quot;, filters = list(geo = &quot;UK&quot;, na_item = &quot;P6&quot;, ## Exports = P6, Imports = P7 unit = &quot;CP_MEUR&quot;, s_adj = &quot;NSA&quot;)) exports.q &lt;- exports.q_raw[,c(&quot;time&quot;, &quot;values&quot;)] plot(exports.q$values, type = &quot;l&quot;) ## Consider the data from 1995:Q1 gdp.q_1995 &lt;- subset(gdp.q, time &gt;= as.Date(&quot;1995-01-01&quot;) &amp; time &lt; as.Date(&quot;2020-01-01&quot;)) gdp.a_1995 &lt;- subset(gdp.a, time &gt;= as.Date(&quot;1995-01-01&quot;) &amp; time &lt; as.Date(&quot;2020-01-01&quot;)) exports.q_1995 &lt;- subset(exports.q, time &gt;= as.Date(&quot;1995-01-01&quot;) &amp; time &lt; as.Date(&quot;2020-01-01&quot;)) par(mfrow = c(2,1)) plot(gdp.q_1995, type = &quot;l&quot;) plt(exports.q_1995, type = &quot;l&quot;) We plot the annual and quarterly GDP series: plot(gdp.a) plot(gdp.q) To perform the disaggregation we use the tempdisaggpackage. Its main function is td(), of which the argument conversion allows to specify wether the resulting disaggregated series should be consistent with either the sum, the average, the first or the last value of low frequency series, to specifies the frequency to which convert the series and method specifies the disaggregation procedure to be employed. Denton-Cholette with no indicator series m1 &lt;- td(gdp.a ~ 1, to = &quot;quarterly&quot;, method = &quot;denton-cholette&quot;) data.q1 &lt;- predict(m1) plot(data.q1) Denton-Cholette with exports as indicator series m2 &lt;- td(gdp.a ~ 0 + exports.q, method = &quot;denton-cholette&quot;) data.q2 &lt;- predict(m2) plot(data.q2) Chow-lin with exports as indicator series m3 &lt;- td(gdp.a ~ exports.q) summary(m3) ## ## Call: ## td(formula = gdp.a ~ exports.q) ## ## Residuals: ## Min 1Q Median 3Q Max ## -177379 -103357 38746 98012 210569 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.066e+05 3.817e+04 2.793 0.0103 * ## exports.q 2.832e+00 2.702e-01 10.478 3.14e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## &#39;chow-lin-maxlog&#39; disaggregation with &#39;sum&#39; conversion ## 25 low-freq. obs. converted to 100 high-freq. obs. ## Adjusted R-squared: 0.8193 AR1-Parameter: 0.8916 data.q3 &lt;- predict(m3) plot(data.q3) Chow-Lin with exports and imports as indicator series: m4 &lt;- td(gdp.a ~ exports.q + imports.q) summary(m4) ## ## Call: ## td(formula = gdp.a ~ exports.q + imports.q) ## ## Residuals: ## Min 1Q Median 3Q Max ## -109890 -44755 -19502 32780 189382 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 119229.917 17137.396 6.957 5.53e-07 *** ## exports.q -4.267 1.286 -3.319 0.00312 ** ## imports.q 6.652 1.210 5.496 1.60e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## &#39;chow-lin-maxlog&#39; disaggregation with &#39;sum&#39; conversion ## 25 low-freq. obs. converted to 100 high-freq. obs. ## Adjusted R-squared: 0.956 AR1-Parameter: 0.6064 data.q5 &lt;- predict(m4) plot(data.q5) Fernandez with exports and imports as indicator series: m5 &lt;- td(gdp.a ~ exports.q + imports.q, method = &quot;fernandez&quot;) summary(m5) ## ## Call: ## td(formula = gdp.a ~ exports.q + imports.q, method = &quot;fernandez&quot;) ## ## Residuals: ## Min 1Q Median 3Q Max ## -87829 -36629 63931 128978 201063 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 81223.664 32659.025 2.487 0.0210 * ## exports.q -1.011 2.090 -0.484 0.6333 ## imports.q 3.747 1.952 1.920 0.0679 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## &#39;fernandez&#39; disaggregation with &#39;sum&#39; conversion ## 25 low-freq. obs. converted to 100 high-freq. obs. ## Adjusted R-squared: 0.6897 AR1-Parameter: 0 data.q5 &lt;- predict(m5) plot(data.q5) Litterman with exports and imports as indicator series: m6 &lt;- td(gdp.a ~ exports.q + imports.q, method = &quot;litterman-maxlog&quot;) summary(m6) ## ## Call: ## td(formula = gdp.a ~ exports.q + imports.q, method = &quot;litterman-maxlog&quot;) ## ## Residuals: ## Min 1Q Median 3Q Max ## -87829 -36629 63931 128978 201063 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 81223.664 32659.025 2.487 0.0210 * ## exports.q -1.011 2.090 -0.484 0.6333 ## imports.q 3.747 1.952 1.920 0.0679 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## &#39;litterman-maxlog&#39; disaggregation with &#39;sum&#39; conversion ## 25 low-freq. obs. converted to 100 high-freq. obs. ## Adjusted R-squared: 0.6897 AR1-Parameter: 0 (truncated) data.q6 &lt;- predict(m6) plot(data.q6) Finally we evaluate the results by comparing them with the true series, by applying a simple squared deviation measure: a &lt;- sum((gdp.q - data.q6)^2) b &lt;- sum((gdp.q - data.q6)^2) c &lt;- sum((gdp.q - data.q6)^2) d &lt;- sum((gdp.q - data.q6)^2) e &lt;- sum((gdp.q - data.q6)^2) f &lt;- sum((gdp.q - data.q6)^2) 3.4 References ESS guidelines on temporal disaggregation, benchmarking and reconciliation, 2018 edition, Eurostat. https://ec.europa.eu/eurostat/documents/3859598/9441376/KS-06-18-355-EN.pdf/fce32fc9-966f-4c13-9d20-8ce6ccf079b6 Sax, Christoph, et al.Â Package tempdisagg, 2020. http://cran.pau.edu.tr/web/packages/tempdisagg/tempdisagg.pdf "],["revisions-analysis.html", "Topic 4 Revisions Analysis 4.1 Classification of revisions 4.2 Definition of revisions 4.3 Descriptive statistics 4.4 An example in R 4.5 References", " Topic 4 Revisions Analysis Economic indicators are often revised many times after their first publication. This is done since Eurostat and the other statistical agencies put special attention on the quality of the data and at the same time try to back the users need of disposing of the most recent data. In particular, there exists a trade-off between the timeliness of data publication, on the one hand, and the reliability and stability of the data on the other. The principles of Eurostats publication policy, which are reported in Eurostats Code of Practice (Eurostat 2017), are intended to balance the need of reliability and accuracy with that of timeliness. As a first step, Eurostat publishes early releases of the statistics, in order to meet the timeliness principle; afterwards, it revises and updates them, as new information becomes available. The publishing of the early versions of the data allow to meet the timeliness principle: as Principle 13 states, European Statistics are released in a timely and punctual manner, and a standard daily time for the release of statistics is made public. Then, to address potential inaccuracies in the early versions, statistical processes are routinely monitored and revised as required and revisions follow standard, well-established and transparent procedures (Principle 8). Revisions are the differences between the early and the latest value of the statistics and can be considered as a measure of the data reliability. Principle 12 in Eurostat (2017) states that the aim of the European Statistics is to  accurately and reliably portray reality, through the integration of different sources of data and revisions which are regularly analysed in order to improve data sources, statistical processes and outputs. 4.1 Classification of revisions Revisions can be carried out for different reasons (Eurostat 2014): Incorporation of additional data: e.g.Â late responses to surveys, replacement of forecasts with available data, incorporation of data which more closely matches concepts and definitions; Updating of routine adjustment/treatment or compilation: e.g.Â adjusting for seasonal factors, changing the base year for the time series; The introduction of new methods and concepts: e.g.Â improvements in the estimation methods, changes in classifications, introduction of new definitions; Correction of errors caused by the incorrect management of source data and/or by wrong answers given by survey respondents. Revisions occur periodically shortly after the first publication, usually after a few months, and are done to add new information and replace forecasts with actual data. Moreover, periodic revisions occur on a longer time span for wider adjustments. When this is the case, the schedule is regular, defined ex-ante and communicated to the public. Instead, in the case of unexpected errors, revisions are carried out as soon as possible, and are therefore occasional and timely. To sum up, we can distinguish different kind of revisions according to their timing (Eurostat 2014): Routine revisions, generally regarding recent periods; Annual revisions, carried out when more information emerges; Major revisions occurring at longer intervals (3/4 years), which depend on the change of classifications, base periods and so on. They require the re-computation of the whole time series; Unexpected revisions, usually done when previous errors are discovered. 4.2 Definition of revisions Given a particular indicator (that is, a time series) and two subsequent estimates of it, which both refer to the same period \\(t\\) (for example, a given month or quarter) a revision can be defined as \\[ R_t = L_t - P_t , \\] where \\(P_t\\) is a a preliminary (or earlier) estimate and \\(L_t\\) a later (more recent) estimate of the indicator. This definition of revision is used when dealing with indicators measured in growth rates (period-on-period or year-on-year growth rates). Instead, when dealing with revisions to values in level, we define revisions in relative terms: \\[ R_t = \\frac{L_t - P_t}{L_t} . \\] ## Real-time datasets A useful tool for producers of official statistics to undertake revision analysis and present its results are the so-called real-time datasets (or revision triangles). These show how estimates change over time and provide further information about the dissemination policy, the timing of revisions, explanation of revision sources and the status of the published data. To construct a real-time dataset, it is necessary to collect several vintages of the same indicator. A vintage is defined as a set of data (sequence of values) that represented the latest estimate for each reference point in the time series at a particular moment in time (Mckenzie and Gamba, 2008). Therefore, a vintage can be thought of as a photograph of the state of the art knowledge on an indicator (relative to a given point in time) taken at a specific point in time. We can define the estimate relative to a particular indicator \\(I\\) as \\[ I_{t,v}, \\] where \\(t\\) represents the point in time to which the indicator refers and \\(v\\) represents the point in time when the vintage that contained that particular estimate of the indicator was released, that is, when the indicator relative to time \\(t\\) was estimated. The real-time dataset is constructed as follows. The revision triangle can be read horizontally, vertically or diagonally. When it is read horizontally, it provides time series forecasts released at the available dates (this can be useful to assess the forecasting models currently used). Instead, when it is read vertically, it gives the revision history referred to a given period \\(t\\), from the preliminary estimates to the latest (such information is useful to assess the reliability of the earlier estimates). Finally, when triangles are read along the main diagonal (or the first sub-diagonal, or the second sub-diagonal, ), they give the time series of the first (or second, or third, ) releases. To be more specific, assuming that \\(v = t\\), the first entry \\(I_{t,v}\\) is the preliminary estimate of the indicator relative to time \\(t\\), while the rest of the first row (in grey) are the forecast made at time \\(v\\) of the same indicator at the following periods \\(t+1\\), \\(t+2\\), \\(...\\) It is easy to understand why it is called triangle, since the grey part of the table represents forecasts and not data that is actually available at the moment of its release. The first column collects the subsequent revisions that have been made to update the estimate of the indicator relative to time \\(t\\). However, most often, the real-time dataset is read diagonally, since this allows to build time series of revisions on which the descriptive analysis is carried out. For example, the main diagonal (in red) collects all the data series when it was first released, the first sub-diagonal (in orange) collects all the data series after it was revised once, and so on. Assuming that the indicator \\(I\\) is collected as growth rates, the revision time series can be easily calculated, according to the formula \\(R_t = L_t - P_t\\), by subtracting horizontally the sub-diagonals to the main diagonal. So, for instance, the first revision will be the time series obtained as \\(\\{(\\text{I}_{t,v}-\\text{I}_{t,v+1}), (\\text{I}_{t+1,v+1}-\\text{I}_{t+1,v+2}), ...\\}\\), the second revision will be obtained as \\(\\{(\\text{I}_{t,v}-\\text{I}_{t,v+2}), (\\text{I}_{t+1,v+1}-\\text{I}_{t+1,v+3}), ...\\}\\), and so on. 4.3 Descriptive statistics Once one has calculated the revisions, it is possible to proceed to the actual analysis of the revisions themselves. This is done by means of a series of descriptive statistics that can be calculated to answer specific questions. In what follows \\(n\\) refers to the number of observations used for the analysis. Mean revision (or arithmetic average). \\[ \\bar{R} = \\frac{1}{n} \\sum_{t=1}^{n} (L_t-P_t) = \\frac{1}{n}\\sum_{t=1}^{n} R_t \\] The sign of this measure indicates wether on average the estimate of the earlier releases is biased. This bias is positive (negative) if the sign of the statistics is negative (positive). However, since revisions of opposite sign cancel out, this measure, often also called average bias, is of limited use. Statistical significance of the mean revision. A modified t-test can be performed to determine whether the mean revision is statistically different from zero, which may give an insight on wether an actual bias exists in the preliminary estimates. The t statistics is computed as \\[ t = \\frac{\\bar{R}}{st.dev(\\bar{R})-HACFormula} \\ , \\] where the denominator is the heteroscedasticity and autocorrelation consistent standard deviation of mean revision and it is defined as the square root of \\(var(\\bar{R})= \\frac{1}{n(n-1)} \\left[ \\sum_{t=1}^{n} \\hat{\\varepsilon}^2_t + \\frac{3}{4} \\sum_{t=2}^{n} \\hat{\\varepsilon}_t\\hat{\\varepsilon}_{t-1} + \\frac{2}{3} \\sum_{t=3}^{n} \\hat{\\varepsilon}_t\\hat{\\varepsilon}_{t-2} \\right]\\) with \\(\\hat{\\varepsilon}_{t} = R_t - \\bar{R} \\ .\\) The critical values of the t statistic and the p-value are computed in the usual way. Median revision. It can be a piece of useful supplementary information to the mean revision as it is not affected by outliers. \\[ Me=Me(R_t) \\] Pecentage of negative/zero/positive revisions. These measure can also be useful supplementary information to the mean revision and can be computed as \\[ 100 \\cdot\\frac{1}{n}\\sum_{t=1}^{n}V_t \\ , \\] where \\(V_t = 1\\) if \\(R_t&lt;0, = 0,&gt;0\\) for negative, zero, positive revisions respectively. Mean absolute revision. \\[ MAR = \\frac{1}{n} \\sum_{t=1}^{n} |L_t-P_t| = \\frac{1}{n}\\sum_{t=1}^{n} |R_t| \\] It indicates the average size of the revisions. However, unlike the mean revision, it does not provide an indication of the directional bias and it is therefore more stable. Range that 90% of the revisions lie within. This is simply the interval of the \\(5^{th}\\) to the \\(95^{th}\\) percentile of the distribution of revisions. It gives information about the expected range the revisions usually lie within. Median absolute revision. It is a measure of central tendency that is little influenced by outliers and can complement the mean absolute revision. \\[ |Me|=Me|R_t| \\] Standard deviation of revisions. \\[ SDR=\\sqrt{\\frac{1}{n-1}\\sum_{t=1}^{n}(R_t-\\bar{R})^2} \\] This measure is used to assess the spread of revisions around their mean. It is sensitive to outliers and therefore it is better not to use it with skewed distributions. It is useful to compare the volatilities of different revision intervals. Root mean square revision. \\[ RMSR=\\sqrt{\\frac{1}{n-1}\\sum_{t=1}^{n}R^2} \\] This is essentially a combination of the mean revision and the variance of revision statistics. It is therefore a broader measure than the standard deviation of revision. Maximum and minimum revision, interquartile deviation and range of revision. These measures can be used to retrieve additional information regarding the distribution of the revisions. Skewness. \\[ SKEW = \\frac{3\\cdot(\\bar{R}-Me)}{SDR} \\] This is a formal measure of asymmetry of the distribution of the revisions. When the statistics is negative (positive) the median is greater (smaller) than the mean and the distribution presents a fatter tail towards the left (right). Correlation between revision and earlier estimate (test if revisions are noise). \\[ \\rho_{R_tP_t} = \\frac{\\sum_{t=1}^{n}(P_t - \\bar{P})(R_t-\\bar{R})}{(n-1) \\cdot \\hat{\\sigma}_P\\hat{\\sigma}_R} \\ , \\] where \\(\\hat{\\sigma}_x = \\sqrt{\\frac{\\sum_{t=1}^{n}(x_t - \\bar{x})}{n-1}} \\ .\\) When the correlation between \\(P_t\\) and \\(R_t\\) is statistically significant, this implies that there was information available at the time of the first estimate which was not efficiently exploited and therefore the revisions can be interpreted as noise. Correlation between revision and earlier estimate (test if revisions are news). \\[ \\rho_{R_tL_t} = \\frac{\\sum_{t=1}^{n}(L_t - \\bar{L})(R_t-\\bar{R})}{(n-1) \\cdot \\hat{\\sigma}_L\\hat{\\sigma}_R} \\] This is in a sense the opposite statistics to the previous one. If the correlation between \\(L_t\\) and \\(R_t\\) is statistically significant, this implies that actual new information is being exploited to compute the revision, and therefore the revision can be interpreted as news. Serial correlation of revisions. \\[ \\rho_{R_tR_t-1} = \\frac{\\sum_{t=2}^{n}(R_{t-1} - \\bar{R})(R_t-\\bar{R})}{(n-1) \\cdot \\hat{\\sigma}_{R_t}\\hat{\\sigma}_{R_{t-1}}} \\] Finally, this correlation measure is used to assess wether there is bias in the revision process (that is, if the revision process is in some way predictable). Decomposition of the Mean Squared Revision. It can be shown that the following holds: \\[ MSR = \\bar{R}^2 + (\\sigma_P - \\rho\\sigma_L)^2 + (1-\\rho^2)\\sigma^2_L, \\] where \\(\\bar{R}\\) is the mean revision, \\(\\sigma_ L\\) and \\(\\sigma_ P\\) are the standard deviations of the latest and preliminary estimates, respectively, and \\(\\rho\\) is their correlation. Dividing by \\(MSR\\) yields \\(1 = UM + UR + UD\\). \\(UM = \\frac{\\bar{R}^2}{MSR}\\) is the proportion of the MSR due to the mean revision being different from zero. It is sometimes called mean error. \\(UR = \\frac{(\\sigma_P - \\rho\\sigma_L)^2}{MSR}\\) is the proportion of MSR which is due to the slope coefficient \\(\\beta\\) being different from \\(0\\), in the linear regression \\(L_t = \\alpha + \\beta P_t + u_t\\). It is sometimes called slope error. \\(UD = \\frac{(1-\\rho^2)\\sigma^2_L}{MSR}\\) is the proportion of MSR which is not caused by systematic differences between earlier and later estimates. A quick interpretation of these measures is that earlier estimates are good if the decomposition gives low values for \\(UM\\) and \\(UR\\) and a high value for \\(UD\\). 4.4 An example in R Revision triangles for the GDP series of the EU and Euro area can be downloaded from Eurostats website: https://ec.europa.eu/eurostat/web/national-accounts/data/other. As an example, the file below reports a revision triangle of the GDP series, and it has been published to demonstrate the reliability of the GDP estimates for the Euro area. The reported revision triangle is relative to the period 2017:Q4-2020:Q3. In particular, this example shows an application to the Quarter on Quarter percentage rates of changes of seasonally and calendar adjusted GDP. library(readxl) ## Import the revision triangle d &lt;- read_xlsx(&quot;RevisionsTriangles_EuroAreaGDP_QonQ.xlsx&quot;) d ## # A tibble: 48 x 13 ## Release `2017Q4` `2018Q1` `2018Q2` `2018Q3` `2018Q4` `2019Q1` `2019Q2` `2019Q3` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2017Q4 Pre~ 0.594 NA NA NA NA NA NA NA ## 2 2017Q4-t+4~ 0.609 NA NA NA NA NA NA NA ## 3 2017Q4-sec~ 0.636 NA NA NA NA NA NA NA ## 4 2017Q4-thi~ 0.699 NA NA NA NA NA NA NA ## 5 2018Q1 Pre~ 0.699 0.488 NA NA NA NA NA NA ## 6 2018Q1-t+4~ 0.699 0.448 NA NA NA NA NA NA ## 7 2018Q1-sec~ 0.706 0.448 NA NA NA NA NA NA ## 8 2018Q1-thi~ 0.697 0.438 NA NA NA NA NA NA ## 9 2018Q2 Pre~ 0.697 0.438 0.425 NA NA NA NA NA ## 10 2018Q2-t+4~ 0.697 0.438 0.443 NA NA NA NA NA ## # ... with 38 more rows, and 4 more variables: 2019Q4 &lt;dbl&gt;, 2020Q1 &lt;dbl&gt;, ## # 2020Q2 &lt;dbl&gt;, 2020Q3 &lt;dbl&gt; As a first step, we organize the data into separate vectors of quarterly estimates containing the revisions. Starting from the first day after the end of the considered quarter, the four estimates in our dataset are: the preliminary estimate (released after 30 days), the flash estimate (after 45 days), a second release (after 65 days) and the last version (after 100 days). We can easily isolate the rows referring to each release by using the function grepl on the first column of d. The four series of quarterly GDP belonging to the first to fourth releases are then obtained by isolating the diagonal of the resulting triangles. ## Data preparation d1 &lt;- d[grepl(&quot;\\\\(T\\\\+30\\\\)&quot;, unlist(d[,1])), ] P &lt;- diag(as.matrix(d1[,-1])) ## Series of the preliminary release d2 &lt;- d[grepl(&quot;\\\\(T\\\\+45\\\\)&quot;, unlist(d[,1])), ] t45 &lt;- diag(as.matrix(d2[,-1])) ## Series of the flash release (after 45 days) d3 &lt;- d[grepl(&quot;\\\\(T\\\\+65\\\\)&quot;, unlist(d[,1])), ] t65 &lt;- diag(as.matrix(d3[,-1])) ## Series of the second release (after 65 days) d4 &lt;- d[grepl(&quot;\\\\(T\\\\+100\\\\)&quot;, unlist(d[,1])), ] L &lt;- diag(as.matrix(d4[,-1])) ## Series of the third release (after 100 days) ## Showing the resulting data data &lt;- cbind(P, t45, t65, L) row.names(data) &lt;- names(d)[-1] data ## P t45 t65 L ## 2017Q4 0.5938556 0.6090199 0.6359723 0.6991634 ## 2018Q1 0.4875134 0.4481298 0.4477231 0.4375630 ## 2018Q2 0.4249304 0.4426700 0.4512978 0.5020015 ## 2018Q3 0.2712428 0.2927501 0.2547051 0.2586501 ## 2018Q4 0.2712863 0.2522401 0.3009477 0.3064836 ## 2019Q1 0.4440005 0.4706097 0.4523143 0.4996077 ## 2019Q2 0.2512180 0.2492598 0.2322116 0.2302577 ## 2019Q3 0.2564727 0.2794697 0.3001252 0.3352565 ## 2019Q4 0.1410930 0.1106100 0.1631295 0.1687033 ## 2020Q1 -3.4673302 -3.3300700 -3.1779150 -3.1757501 ## 2020Q2 -11.9037191 -11.6810502 -11.4169745 -11.4304857 ## 2020Q3 12.0830028 11.6156200 11.5484497 11.5384223 For illustrative purposes, we study the first three series of revisions. ## Isolating the relevant revisions R1 &lt;- t45-P R2 &lt;- t65-t45 R3 &lt;- L-t65 n &lt;- length(R1) We can now proceed with the actual analysis of the revisions. In doing so, we try to answer some specific questions. What is the average size of revisions, or the usual range that revisions lie within? ## Mean absolute revision MAR1 &lt;- sum(abs(R1))/n MAR2 &lt;- sum(abs(R2))/n MAR3 &lt;- sum(abs(R3))/n ## Range that 90% of revisions lie within range90_1 &lt;- quantile(R1,0.95) - quantile(R1,0.05) range90_2 &lt;- quantile(R2,0.95) - quantile(R2,0.05) range90_3 &lt;- quantile(R3,0.95) - quantile(R3,0.05) ## Median absolute revision abs.median1 &lt;- median(abs(R1)) abs.median2 &lt;- median(abs(R2)) abs.median3 &lt;- median(abs(R3)) ## Organize the results in a table table1 &lt;- cbind( rbind(MAR1, MAR2, MAR3), rbind(abs.median1, abs.median2, abs.median3), rbind(range90_1, range90_2, range90_3)) colnames(table1) &lt;- c(&quot;Mean absolute revision&quot;, &quot;Median absolute revision&quot;, &quot;90% range&quot;) rownames(table1) &lt;- c(&quot;Revision 1&quot;, &quot;Revision 2&quot;, &quot;Revision 3&quot;) table1 ## Mean absolute revision Median absolute revision 90% range ## Revision 1 0.08518336 0.02480314 0.40767741 ## Revision 2 0.05955491 0.03249872 0.25367064 ## Revision 3 0.02076598 0.01009372 0.06799115 As we can easily observe by looking at table1, the first revision is the most substantial, as it is natural to expect. Subsequent revisions tend to introduce slighter corrections in the estimates. We can also see that later revisions tend to be more concentrated in a smaller interval for each data point. Moreover, in all three revisions, the median absolute revision is smaller than the mean, suggesting that some outliers (bigger revisions) occur that drive the mean upwards. Is the average level of revision close to zero, or is there an indication that a possible bias exists in the earlier estimate? ## Mean revision MR1 &lt;- sum(R1)/n MR2 &lt;- sum(R2)/n MR3 &lt;- sum(R3)/n ## Median revision median1 &lt;- median(R1) median2 &lt;- median(R2) median3 &lt;- median(R3) ## % of positive/zero/negative revisions neg1 &lt;- sum(R1 &lt; -0.005)/n zer1 &lt;- sum(-0.005 &lt; R1 &amp; R1 &lt; 0.005)/n pos1 &lt;- sum(R1 &gt; 0.005)/n neg2 &lt;- sum(R2 &lt; -0.005)/n zer2 &lt;- sum(-0.005 &lt; R2 &amp; R2 &lt; 0.005)/n pos2 &lt;- sum(R2 &gt; 0.005)/n neg3 &lt;- sum(R3 &lt; -0.005)/n zer3 &lt;- sum(-0.005 &lt; R3 &amp; R3 &lt; 0.005)/n pos3 &lt;- sum(R3 &gt; 0.005)/n ## Testing whether the mean revision is statistically different from zero t.test_adj &lt;- function(R){ n &lt;- length(R) e &lt;- R - mean(R) varR &lt;- (sum(e^2) + 0.75*(sum(e[2:n]*e[1:(n-1)])) + (2/3)*sum(e[3:n]*e[1:(n-2)]))/(n*(n-1)) t_stat &lt;- mean(R)/sqrt(varR) p_value = 2*pt(abs(t_stat), n-1, lower.tail=F) list(t_stat, p_value) } ## Table of results table2 &lt;- cbind( rbind(t.test_adj(R1)[1], t.test_adj(R2)[1], t.test_adj(R3)[1]), rbind(t.test_adj(R1)[2], t.test_adj(R2)[2], t.test_adj(R3)[2]), rbind(neg1, neg2, neg3), rbind(zer1, zer2, zer3), rbind(pos1, pos2, pos3)) colnames(table2) &lt;- c(&quot;t-statistc&quot;, &quot;p-value&quot;, &quot;Negative %&quot;, &quot;Close to zero %&quot;, &quot;Positive %&quot;) rownames(table2) &lt;- c(&quot;Revision 1&quot;, &quot;Revision 2&quot;, &quot;Revision 3&quot;) table2 ## t-statistc p-value Negative % Close to zero % Positive % ## Revision 1 -0.2077315 0.8392345 0.3333333 0.08333333 0.5833333 ## Revision 2 1.344024 0.2060048 0.3333333 0.08333333 0.5833333 ## Revision 3 1.944858 0.07779569 0.25 0.25 0.5 As the t-statistics in table2 shows, there is no evidence of any significant deviation of the average revision from zero at a 95% confidence level. This result supports the reliability of Eurostats estimates for the GDP of the Euro area. However, the percentage of positive revisions in our sample tends to be quite higher that the percentage of negative revisions. This could indicate either that the preliminary estimate is negatively biased or that the revision process is positively biased. What is the extent of variability of revisions? ## Standard deviation of revision SDR1 &lt;- sd(R1) SDR2 &lt;- sd(R2) SDR3 &lt;- sd(R3) ## Root mean square revision RMSR1 &lt;- sqrt(sum(R1^2)/(n-1)) RMSR2 &lt;- sqrt(sum(R2^2)/(n-1)) RMSR3 &lt;- sqrt(sum(R3^2)/(n-1)) ## Skewness skew1 &lt;- 3*(MR1-median1)/SDR1 skew2 &lt;- 3*(MR2-median2)/SDR2 skew3 &lt;- 3*(MR3-median3)/SDR3 ## Table of results table3 &lt;- cbind( rbind(SDR1, SDR2, SDR3), rbind(RMSR1, RMSR2, RMSR3), rbind(skew1, skew2, skew3)) colnames(table3) &lt;- c(&quot;Std. dev. of revision&quot;, &quot;Root mean square revision&quot;, &quot;Skewness of revision&quot;) rownames(table3) &lt;- c(&quot;Revision 1&quot;, &quot;Revision 2&quot;, &quot;Revision 3&quot;) table3 ## Std. dev. of revision Root mean square revision Skewness of revision ## Revision 1 0.16270955 0.16291646 -0.4482379 ## Revision 2 0.09056674 0.09808630 0.7095003 ## Revision 3 0.02674799 0.03090598 1.1309405 We can see that the later releases are more concentrated around their mean and we can see this from both the standard deviation of revision and the root mean square revision. Furthermore, the distributions of the revisions are not entirely symmetric. In particular, the first revision stands out as being negatively skewed while the third revision as being positively skewed. As a general rule of thumb: if the skewness is less than -1 or greater than 1, the distribution is highly skewed; if the skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed; if the skewness is between -0.5 and 0.5, the distribution is approximately symmetric. What is the average size of revision relative to the estimate itself? ## Relative mean absolute revision RMAR1 &lt;- sum(abs(R1))/sum(abs(P)) RMAR2 &lt;- sum(abs(R2))/sum(abs(P)) RMAR3 &lt;- sum(abs(R3))/sum(abs(P)) ## Average absolute value of first published estimate p_bar &lt;- sum(abs(P))/n ## Table of results table4 &lt;- rbind(RMAR1, RMAR2, RMAR3) colnames(table4) &lt;- &quot;Relative mean absolute revision&quot; rownames(table4) &lt;- c(&quot;Revision 1&quot;, &quot;Revision 2&quot;, &quot;Revision 3&quot;) table4 ## Relative mean absolute revision ## Revision 1 0.033409974 ## Revision 2 0.023358175 ## Revision 3 0.008144674 The mean absolute revision can be interpreted as the expected proportion of the first published estimate that is likely to be revised over the revision interval being considered. Therefore, within 45 days, around 3% of the estimetes are likely to be revised and in the subsequent 55 days only around an additional 3%. Is the earlier published estimate a good or efficient forecast of the later published estimate? ## Test if revisions are &quot;noise&quot; rho1 &lt;- cor(R1,P) rho2 &lt;- cor(R2,P) rho3 &lt;- cor(R3,P) ## Test if revisions are &quot;news&quot; rhor1 &lt;- cor(R1,L) rhor2 &lt;- cor(R2,L) rhor3 &lt;- cor(R3,L) ## Serial correlation of revisions ser.cor1 &lt;- cor(R1[2:12],R1[1:11]) ser.cor2 &lt;- cor(R2[2:12],R2[1:11]) ser.cor3 &lt;- cor(R3[2:12],R3[1:11]) ## Mean squared revision MSR1 &lt;- sum(R1^2)/(n-1) MSR2 &lt;- sum(R2^2)/(n-1) MSR3 &lt;- sum(R3^2)/(n-1) ## Table of results table5 &lt;- cbind( rbind(rho1, rho2, rho2), rbind(rhor1, rhor2, rhor3), rbind(ser.cor1, ser.cor2, ser.cor3), rbind(MSR1, MSR2, MSR3) ) colnames(table5) &lt;- c(&quot;Noise&quot;, &quot;News&quot;, &quot;Serial correlation&quot;, &quot;MSR&quot;) rownames(table5) &lt;- c(&quot;Revision 1&quot;, &quot;Revision 2&quot;, &quot;Revision 3&quot;) table5 ## Noise News Serial correlation MSR ## Revision 1 -0.9361141 -0.93279041 -0.5760142 0.0265417738 ## Revision 2 -0.8548409 -0.85445458 0.1182194 0.0096209214 ## Revision 3 -0.8548409 0.08842704 -0.4008581 0.0009551794 We do not find evidence that the revision process is driven by noise nor by news. Rather, since the revisions and the earlier estimates are negatively correlated, it appears that the revision process is actually smoothing out the initial estimates. That is, if a given estimate was initially particularly high, the revision process tends to lower that estimate and does the converse for particularly low initial estimates. Futhermore, there is no strong serial correlation, so that there appears not to be strong biases in the revision process. 4.5 References McKenzie, Richard, and Michela Gamba (2008). Interpreting the results of Revision Analyses: Recommended Summary Statistics. Contribution to OECD/Eurostat Task Force on Performing Revisions Analysis for Sub-Annual Economic Statistics. Eurostat (2014), Memobust Handbook on Methodology of Modern Business Statistics. https://ec.europa.eu/eurostat/cros/system/files/Quality Aspects-02-T-Revisions of Economic Official Statistics v1.0.pdf Eurostat (2017). European Statistics Code of Practice. https://ec.europa.eu/eurostat/documents/4031688/8971242/KS-02-18-142-EN-N.pdf/e7f85f07-91db-4312-8118-f729c75878c7?t=1528447068000 ESS guidelines on revision policy for PEEIs, 2013 edition. https://ec.europa.eu/eurostat/documents/3859598/5935517/KS-RA-13-016-EN.PDF.pdf/42d365e5-8a65-42f4-bc0b-aacb02c93cf7?t=1558683870000 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
